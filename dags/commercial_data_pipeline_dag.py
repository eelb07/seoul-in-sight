import datetime
import decimal
import hashlib
import json
import logging
import os
import time
import traceback
from io import BytesIO

import boto3
import botocore.exceptions
import pandas as pd
import pendulum
import psutil # ì‹œìŠ¤í…œ ìœ í‹¸ë¦¬í‹° (ì‚¬ìš©ë˜ì§€ ì•Šì„ ì‹œ ì œê±°)
import psycopg2
import pyarrow as pa
import pyarrow.parquet as pq

from airflow.decorators import dag, task
from airflow.hooks.base import BaseHook
from airflow.models import Variable



BUCKET_NAME = Variable.get("BUCKET_NAME")
S3_PREFIX = Variable.get("S3_PREFIX")
S3_PQ_PREFIX_COMM = Variable.get("S3_PQ_PREFIX_COMM")
S3_PQ_PREFIX_RSB = Variable.get("S3_PQ_PREFIX_RSB")
S3_PROCESSED_HISTORY_PREFIX = Variable.get("S3_PROCESSED_HISTORY_PREFIX")
REDSHIFT_IAM_ROLE = Variable.get("REDSHIFT_IAM_ROLE_ARN")


log = logging.getLogger(__name__)

def generate_source_id(area_code: str, observed_at: str) -> str:
    raw = f"{area_code}_{observed_at}"
    return hashlib.sha256(raw.encode()).hexdigest()[:32]  # 32ìë¦¬ ê³ ì • ê¸¸ì´

def parse_int(val):
    try:
        return int(float(val)) if val not in [None, ""] else pd.NA
    except:
        return pd.NA

def parse_float(val):
    try:
        return float(val)
    except (ValueError, TypeError):
        return None
    
def upload_processed_history_to_s3(
    s3_client,
    bucket_name: str,
    s3_key: str, 
    processed_history_data: dict # ì—…ë¡œë“œí•  processed_observed_at_dict ë°ì´í„°
):
    """
    ì—…ë°ì´íŠ¸ëœ ì²˜ë¦¬ ì´ë ¥ ë”•ì…”ë„ˆë¦¬ (processed_observed_at_dict)ë¥¼ S3ì— JSON íŒŒì¼ë¡œ ì—…ë¡œë“œí•©ë‹ˆë‹¤.

    Args:
        s3_client: ì´ˆê¸°í™”ëœ boto3 S3 í´ë¼ì´ì–¸íŠ¸ ê°ì²´.
        bucket_name (str): S3 ë²„í‚· ì´ë¦„.
        s3_key (str): ì²˜ë¦¬ ì´ë ¥ íŒŒì¼ì´ ì €ì¥ë  S3 í‚¤ (ì˜ˆ: "history/processed_observations.json").
        processed_history_data (dict): area_id ë³„ë¡œ ê·¸ë£¹í™”ëœ ì²˜ë¦¬ ì´ë ¥ ë°ì´í„° (processed_observed_at_dict).
    """
    try:
        updated_content_json_string = json.dumps(processed_history_data, indent=4, ensure_ascii=False)

        s3_client.put_object(
            Bucket=bucket_name,
            Key=s3_key,
            Body=updated_content_json_string.encode('utf-8'),
            ContentType='application/json' 
        )
        
        total_records_count = sum(len(observations) for observations in processed_history_data.values())
        log.info(f"âœ… S3ì— {total_records_count}ê°œì˜ ì²˜ë¦¬ ì´ë ¥ì„ ì„±ê³µì ìœ¼ë¡œ ì—…ë°ì´íŠ¸í–ˆìŠµë‹ˆë‹¤: s3://{bucket_name}/{s3_key}")

    except Exception as e:
        log.error(f"âŒ ì²˜ë¦¬ ì´ë ¥ íŒŒì¼ì„ S3ì— ì €ì¥í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤ (s3://{bucket_name}/{s3_key}): {e}")
        raise




default_args = {
    "owner": "seungalee", 
    "email": ["teamfirst.dag.alert@gmail.com"], # ì•Œë¦¼ì„ ë°›ì„ ì´ë©”ì¼ ì£¼ì†Œ ëª©ë¡
    "email_on_failure": True, 
}

@dag(
    dag_id="dag_commercial_v1",
    schedule="*/5 * * * *",
    start_date=pendulum.datetime(2025, 7, 2, tz="Asia/Seoul"),
    catchup=False,
    doc_md="""
    # ìƒê¶Œ ë°ì´í„° ETL íŒŒì´í”„ë¼ì¸
    - task1 :
    - task2 :
    """,
    tags=["seoul", "commercial", "ETL"],
    default_args=default_args
)
def commercial_data_pipeline():
    @task(task_id="extract_and_transfrom")
    def extract_and_transform(s3_client):

        process_start_time = pendulum.now(tz="Asia/Seoul")
        start_time_for_files = process_start_time.subtract(minutes=5)
        log.info(f"ğŸ””{start_time_for_files} ~ {process_start_time} ì‚¬ì´ì˜ raw_json ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")

        # ì²˜ë¦¬í•´ì•¼í•  ì „ì²´ íŒŒì¼ ê²½ë¡œë¥¼ ì •ì˜ í•©ë‹ˆë‹¤. 
        files_to_process = []
        for i in range(5):
            file_time = start_time_for_files.add(minutes=i)
            file_time_HHmm = file_time.strftime("%H%M")

            for area_id in range(82):
                files_to_process.append({
                    'file_time': file_time,
                    'area_id': area_id,
                    'file_name': f"{file_time_HHmm}_{area_id}.json"
                })
        # S3ì—ì„œ ê¸°ì¡´ ì²˜ë¦¬ ì´ë ¥ ë¡œë“œ
        processed_history_s3_key = f"{S3_PROCESSED_HISTORY_PREFIX}/processed_observations.json"
        processed_observed_at_set = set()
        processed_observed_at_dict = {}
        try:
            response = s3_client.get_object(Bucket=BUCKET_NAME, Key=processed_history_s3_key)
            processed_observed_at_dict = json.loads(response["Body"].read())

            for area_id, values in processed_observed_at_dict.items():
                area_id = int(area_id)
                for value in values:
                    observed_at = value['observed_at']
                    processed_observed_at_set.add((area_id, observed_at))  # setì„ ì´ìš©í•œ ì¡°íšŒë¥¼ ìœ„í•´, jsonì—ì„œ ì´ë ¥ì„ ëª¨ë‘ ì½ì–´ì™€ (area_id, observed_at) íŠœí”Œë¡œ êµ¬ì„±ëœ setì„ ì´ˆê¸°í™” 
            log.info(f"ğŸ”” S3ì—ì„œ {len(processed_observed_at_set)}ê°œì˜ ê¸°ì¡´ ì²˜ë¦¬ ì´ë ¥ì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")

        except botocore.exceptions.ClientError as e:
            if e.response['Error']['Code'] == "NoSuchKey": # ì²˜ë¦¬ ì´ë ¥ íŒŒì¼(json)ì´ ì—†ëŠ” ê²½ìš°, ì¦‰ ì²« ì‹œì‘.
                log.warning(f"ğŸš¨ {processed_history_s3_key} ê²½ë¡œì— ê¸°ì¡´ ì²˜ë¦¬ ì´ë ¥ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")
            else:
                raise 

        def is_processed(area_id: int, observed_at: str) -> bool:
            "í•´ë‹¹ area_idì™€ observed_at ì¡°í•©ì´ ì´ë¯¸ ì²˜ë¦¬ ë˜ì—ˆì§€ ì—¬ë¶€ë¥¼ ë°˜í™˜"
            return (area_id, observed_at) in processed_observed_at_set  
    
        source_commercial_data = []
        source_commercial_rsb_data = []

        for file_info in files_to_process:
            file_time = file_info['file_time']  # 2025-07-03 00:00:00+09:00
            area_id = file_info['area_id']
            file_name = file_info['file_name']

            key = f"{S3_PREFIX}/{file_time.strftime('%Y%m%d')}/{file_name}"  # 20250703/0000_{area_id}.json

            try:
                response = s3_client.get_object(Bucket=BUCKET_NAME, Key=key)
                content = response["Body"].read()  
                raw_data = json.loads(content)
                raw_all_commercial_data = raw_data['LIVE_CMRCL_STTS']
                raw_observed_at = raw_all_commercial_data.get("CMRCL_TIME") # 20250702 2340
                
                try: # 20250702 2340 (STR) -> 2025-07-02 23:40:00 (DATETIME)
                    observed_at = pendulum.from_format(raw_observed_at, "YYYYMMDD HHmm", tz="Asia/Seoul").format("YYYY-MM-DD HH:mm:ss")
                except Exception as e:
                    log.error(f"ğŸš¨ í•´ë‹¹ ì‹œê°({raw_observed_at}) íŒŒì‹±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. - ì˜¤ë¥˜: {e} ")
                    continue
                
                if is_processed(area_id=area_id, observed_at=observed_at):
                    log.info(f"â­ï¸ í•´ë‹¹ ì‹œê°ì˜ ìƒê¶Œ ë°ì´í„°ëŠ” ì´ë¯¸ ì²˜ë¦¬ ë˜ì—ˆìŠµë‹ˆë‹¤. ìŠ¤í‚µí•©ë‹ˆë‹¤. : {file_name} (area_id: {area_id}, observed_at: {observed_at})")
                    continue

                source_id = generate_source_id(raw_data.get("AREA_CD", ""), observed_at)

                source_commercial_data.append({
                    'source_id': source_id,
                    'area_code': str(raw_data.get("AREA_CD", "")),
                    'area_name': str(raw_data.get("AREA_NM", "")),
                    'congestion_level': str(raw_all_commercial_data.get("AREA_CMRCL_LVL", "")),                         # ì¥ì†Œ ì‹¤ì‹œê°„ ìƒê¶Œ í˜„í™© 
                    'total_payment_count': parse_int(raw_all_commercial_data.get("AREA_SH_PAYMENT_CNT")),               # ì¥ì†Œ ì‹¤ì‹œê°„ ì‹ í•œì¹´ë“œ ê²°ì œ ê±´ìˆ˜    -- INT32
                    'payment_amount_min': parse_int(raw_all_commercial_data.get("AREA_SH_PAYMENT_AMT_MIN")),            # ì¥ì†Œ ì‹¤ì‹œê°„ ì‹ í•œì¹´ë“œ ê²°ì œ ìµœì†Œê°’  -- INT32
                    'payment_amount_max': parse_int(raw_all_commercial_data.get("AREA_SH_PAYMENT_AMT_MAX")),            # ì¥ì†Œ ì‹¤ì‹œê°„ ì‹ í•œì¹´ë“œ ê²°ì œ ìµœëŒ€ê°’  -- INT32
                    'male_ratio': parse_float(raw_all_commercial_data.get("CMRCL_MALE_RATE")),                          # ë‚¨ì„± ì†Œë¹„ ë¹„ìœ¨
                    'female_ratio': parse_float(raw_all_commercial_data.get("CMRCL_FEMALE_RATE")),                      # ì—¬ì„± ì†Œë¹„ ë¹„ìœ¨
                    'age_10s_ratio': parse_float(raw_all_commercial_data.get("CMRCL_10_RATE")),                         # 10ëŒ€ ì†Œë¹„ ë¹„ìœ¨
                    'age_20s_ratio': parse_float(raw_all_commercial_data.get("CMRCL_20_RATE")),                         # 20ëŒ€ ì†Œë¹„ ë¹„ìœ¨
                    'age_30s_ratio': parse_float(raw_all_commercial_data.get("CMRCL_30_RATE")),                         # 30ëŒ€ ì†Œë¹„ ë¹„ìœ¨
                    'age_40s_ratio': parse_float(raw_all_commercial_data.get("CMRCL_40_RATE")),                         # 40ëŒ€ ì†Œë¹„ ë¹„ìœ¨
                    'age_50s_ratio': parse_float(raw_all_commercial_data.get("CMRCL_50_RATE")),                         # 50ëŒ€ ì†Œë¹„ ë¹„ìœ¨
                    'age_60s_ratio': parse_float(raw_all_commercial_data.get("CMRCL_60_RATE")),                         # 60ëŒ€ ì†Œë¹„ ë¹„ìœ¨
                    'individual_consumer_ratio': parse_float(raw_all_commercial_data.get("CMRCL_PERSONAL_RATE")),       # ê°œì¸ ì†Œë¹„ ë¹„ìœ¨
                    'corporate_consumer_ratio': parse_float(raw_all_commercial_data.get("CMRCL_CORPORATION_RATE")),     # ë²•ì¸ ì†Œë¹„ ë¹„ìœ¨
                    'observed_at': observed_at,                                                                         # ì‹¤ì‹œê°„ ìƒê¶Œ ì—…ë°ì´íŠ¸ ì‹œê°„ 
                    'created_at': pendulum.now("Asia/Seoul").to_datetime_string()                                       # ì ì¬ ì‹œê°„
                })

                for idx, value in enumerate(raw_all_commercial_data.get("CMRCL_RSB", [])): # ì—¬ëŸ¬ ì¹´í…Œê³ ë¦¬ì˜ ìƒê¶Œ ë°ì´í„°ê°€ ì¡´ì¬ 
                    source_commercial_rsb_data.append({
                        # 'area_id': area_id,
                        'source_id': source_id,
                        'category_large': str(value.get("RSB_LRG_CTGR", "")),                                           # ì—…ì¢… ëŒ€ë¶„ë¥˜
                        'category_medium': str(value.get("RSB_MID_CTGR", "")),                                          # ì—…ì¢… ì¤‘ë¶„ë¥˜
                        'category_congestion_level': str(value.get("RSB_PAYMENT_LVL", "")),                             # ì—…ì¢… ì‹¤ì‹œê°„ ìƒê¶Œ í˜„í™©
                        'category_payment_count': parse_int(value.get("RSB_SH_PAYMENT_CNT")),                           # ì—…ì¢… ì‹¤ì‹œê°„ ì‹ í•œì¹´ë“œ ê²°ì œ ê±´ìˆ˜    -- INT32
                        'category_payment_min': parse_int(value.get("RSB_SH_PAYMENT_AMT_MIN")),                         # ì—…ì¢… ì‹¤ì‹œê°„ ì‹ í•œì¹´ë“œ ê²°ì œ ê¸ˆì•¡ ìµœì†Œê°’ -- INT32
                        'category_payment_max': parse_int(value.get("RSB_SH_PAYMENT_AMT_MAX")),                         # ì—…ì¢… ì‹¤ì‹œê°„ ì‹ í•œì¹´ë“œ ê²°ì œ ê¸ˆì•¡ ìµœëŒ€ê°’ -- INT32
                        'merchant_count': parse_int(value.get("RSB_MCT_CNT")),                                          # ì—…ì¢… ê°€ë§¹ì  ìˆ˜ -- INT32
                        'merchant_basis_month': str(value.get("RSB_MCT_TIME", "")),                                     # ì—…ì¢… ê°€ë§¹ì  ìˆ˜ ì—…ë°ì´íŠ¸ ì›”    -- CHAR(6)
                        'observed_at': observed_at,                                                                     # ì‹¤ì‹œê°„ ìƒê¶Œ í˜„í™© ì—…ë°ì´íŠ¸ ì‹œê°„
                        'created_at': pendulum.now("Asia/Seoul").to_datetime_string()                                   # ì ì¬ì‹œê°„ 
                    })
                log.info(f"ğŸ”” {file_name}ì˜ ì „ì²˜ë¦¬ë¥¼ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.")


                # ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ëœ ë°ì´í„°ëŠ” ì´ë ¥ setì— ì¶”ê°€
                processed_observed_at_set.add((area_id, observed_at))
                # S3ì— ì €ì¥í•  ì´ë ¥ ë”•ì…”ë„ˆë¦¬ì—ë„ ì¶”ê°€
                if str(area_id) not in processed_observed_at_dict:
                    processed_observed_at_dict[str(area_id)] = []
                processed_observed_at_dict[str(area_id)].append({
                    'observed_at': observed_at,
                    'processed_at': pendulum.now("Asia/Seoul").to_datetime_string()
                })

            except botocore.exceptions.ClientError as e:
                if e.response['Error']['Code'] == "NoSuchKey":
                    log.info(f"ğŸš¨ í•´ë‹¹ ì‹œê°({key})ì˜ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
                    continue
                else:
                    raise 

            
        return {
            'source_commercial_data' : source_commercial_data,
            'source_commercial_rsb_data' : source_commercial_rsb_data,
            'processed_observed_at_dict' : processed_observed_at_dict

        }
    
    @task(task_id="upload_parquet")
    def upload_parquet_to_s3(data_dict: dict, s3_client):
        commercial_data = data_dict['source_commercial_data']
        commercial_rsb_data = data_dict['source_commercial_rsb_data']

        current_process_time = pendulum.now("Asia/Seoul") 
        date_yyyymmdd = current_process_time.strftime('%Y%m%d')
        time_hhmm = current_process_time.strftime('%H%M') 

        saved_parquet_paths = {
            'commercial_parquet_path': None,
            'rsb_parquet_path': None
        }

        if commercial_data:
            # ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ë¥¼ PyArrow Table ìƒì„±ì„ ìœ„í•œ ì»¬ëŸ¼ë³„ ë¦¬ìŠ¤íŠ¸(dict of lists)ë¡œ ë³€í™˜
            # 'source_id'ëŠ” Redshiftì—ì„œ IDENTITY(1,1)ì´ë¯€ë¡œ Parquet íŒŒì¼ì— í¬í•¨í•˜ì§€ ì•ŠëŠ”ë‹¤.
            
            columns_data = {
                'source_id': [],
                'area_code': [],
                'area_name': [],
                'congestion_level': [],
                'total_payment_count': [],
                'payment_amount_min': [],
                'payment_amount_max': [],
                'male_ratio': [],
                'female_ratio': [],
                'age_10s_ratio': [],
                'age_20s_ratio': [],
                'age_30s_ratio': [],
                'age_40s_ratio': [],
                'age_50s_ratio': [],
                'age_60s_ratio': [],
                'individual_consumer_ratio': [],
                'corporate_consumer_ratio': [],
                'observed_at': [],
                'created_at': []
            }
            
            # ë°ì´í„° íƒ€ì… ë§¤í•‘ ë° ì²˜ë¦¬
            for row in commercial_data:
                # ë¬¸ìì—´ ì»¬ëŸ¼
                columns_data['source_id'].append(str(row.get("source_id", "")))
                columns_data['area_code'].append(str(row.get("area_code", "")))
                columns_data['area_name'].append(str(row.get("area_name", "")))
                columns_data['congestion_level'].append(str(row.get("congestion_level", "")))
                
                columns_data['total_payment_count'].append(row.get("total_payment_count"))
                columns_data['payment_amount_min'].append(row.get("payment_amount_min"))
                columns_data['payment_amount_max'].append(row.get("payment_amount_max"))
                
                for col in [
                    "male_ratio", "female_ratio", "age_10s_ratio", "age_20s_ratio",
                    "age_30s_ratio", "age_40s_ratio", "age_50s_ratio", "age_60s_ratio", 
                    "individual_consumer_ratio", "corporate_consumer_ratio"
                ]:
                    val = row.get(col)
                    if pd.isna(val) or val is None: 
                        columns_data[col].append(None)
                    else:
                        columns_data[col].append(decimal.Decimal(str(round(float(val), 1))))

                try:
                    columns_data['observed_at'].append(pd.Timestamp(row.get("observed_at")))
                except (ValueError, TypeError):
                    columns_data['observed_at'].append(None)
                
                try:
                    columns_data['created_at'].append(pd.Timestamp(row.get("created_at")))
                except (ValueError, TypeError):
                    columns_data['created_at'].append(None)


            # PyArrow Schema ì •ì˜
            schema = pa.schema([
                pa.field('source_id', pa.string()),
                pa.field('area_code', pa.string()),
                pa.field('area_name', pa.string()),
                pa.field('congestion_level', pa.string()),
                pa.field('total_payment_count', pa.int32()),       # Redshift BIGINTì— ë§¤í•‘ -> int32
                pa.field('payment_amount_min', pa.int32()),        
                pa.field('payment_amount_max', pa.int32()),        
                pa.field('male_ratio', pa.decimal128(5, 2)),       
                pa.field('female_ratio', pa.decimal128(5, 2)),      
                pa.field('age_10s_ratio', pa.decimal128(5, 2)),     
                pa.field('age_20s_ratio', pa.decimal128(5, 2)),    
                pa.field('age_30s_ratio', pa.decimal128(5, 2)),    
                pa.field('age_40s_ratio', pa.decimal128(5, 2)),    
                pa.field('age_50s_ratio', pa.decimal128(5, 2)),    
                pa.field('age_60s_ratio', pa.decimal128(5, 2)),    
                pa.field('individual_consumer_ratio', pa.decimal128(5, 2)),
                pa.field('corporate_consumer_ratio', pa.decimal128(5, 2)), 
                pa.field('observed_at', pa.timestamp('s')),       
                pa.field('created_at', pa.timestamp('s'))         
            ])
            
            # PyArrow Table ìƒì„± (from_pydict ì‚¬ìš©)
            # from_pydictëŠ” í‚¤-ê°’ ìŒì˜ ë”•ì…”ë„ˆë¦¬ì—ì„œ í…Œì´ë¸”ì„ ìƒì„±í•¨.
            table = pa.Table.from_pydict(columns_data, schema=schema)
                
            # Parquet ì €ì¥
            buffer_commercial = BytesIO()
            pq.write_table(table, buffer_commercial, compression='snappy')
            
            # S3 ì—…ë¡œë“œ
            s3_key_commercial = f"{S3_PQ_PREFIX_COMM}/{date_yyyymmdd}/{time_hhmm}.parquet"
            
            s3_client.put_object(
                Bucket=BUCKET_NAME,
                Key=s3_key_commercial,
                Body=buffer_commercial.getvalue(),
                ContentType='application/octet-stream'
            )
            print(f"ğŸ”” ìƒê¶Œ ë°ì´í„°ë¥¼ ì €ì¥ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤. : s3://{BUCKET_NAME}/{s3_key_commercial}")
            saved_parquet_paths['commercial_parquet_path'] = f"s3://{BUCKET_NAME}/{s3_key_commercial}"

        else:
            print("ğŸš¨ ì²˜ë¦¬í•  ìƒê¶Œ ì¼ë°˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")


        if commercial_rsb_data:
            columns_data_rsb = {
                'source_id' : [],
                'category_large': [],
                'category_medium': [],
                'category_congestion_level' : [],
                'category_payment_count': [],
                'category_payment_min': [],
                'category_payment_max': [],
                'merchant_count': [],
                'merchant_basis_month' : [],
                'observed_at': [],
                'created_at': []
            }

            for row in commercial_rsb_data:
                columns_data_rsb['source_id'].append(str(row.get('source_id', '')))
                columns_data_rsb['category_large'].append(str(row.get('category_large', '')))
                columns_data_rsb['category_medium'].append(str(row.get('category_medium', '')))
                columns_data_rsb['category_congestion_level'].append(str(row.get('category_congestion_level', '')))
                
                columns_data_rsb['category_payment_count'].append(row.get('category_payment_count'))
                columns_data_rsb['category_payment_min'].append(row.get('category_payment_min'))
                columns_data_rsb['category_payment_max'].append(row.get('category_payment_max'))
                columns_data_rsb['merchant_count'].append(row.get('merchant_count'))
                columns_data_rsb['merchant_basis_month'].append(row.get('merchant_basis_month'))

                try:
                    columns_data_rsb['observed_at'].append(pd.Timestamp(row.get('observed_at')))
                except (ValueError, TypeError):
                    columns_data_rsb['observed_at'].append(None)
                
                try:
                    columns_data_rsb['created_at'].append(pd.Timestamp(row.get('created_at')))
                except (ValueError, TypeError):
                    columns_data_rsb['created_at'].append(None)

            # PyArrow Schema ì •ì˜
            schema_rsb = pa.schema([
                pa.field('source_id', pa.string()),
                pa.field('category_large', pa.string()),
                pa.field('category_medium', pa.string()),
                pa.field('category_congestion_level', pa.string()),
                pa.field('category_payment_count', pa.int32()), 
                pa.field('category_payment_min', pa.int32()),   
                pa.field('category_payment_max', pa.int32()),   
                pa.field('merchant_count', pa.int32()),         
                pa.field('merchant_basis_month', pa.string()),
                pa.field('observed_at', pa.timestamp('s')),   # Redshift TIMESTAMP -> pa.timestamp('ns')
                pa.field('created_at', pa.timestamp('s'))      # Redshift TIMESTAMP -> pa.timestamp('ns')
            ])

            # PyArrow Table ìƒì„±
            table_rsb = pa.Table.from_pydict(columns_data_rsb, schema=schema_rsb)

            # Parquet íŒŒì¼ë¡œ ì €ì¥
            buffer_rsb = BytesIO()
            pq.write_table(table_rsb, buffer_rsb, compression='snappy')
            
            # S3ì— ì—…ë¡œë“œ
            s3_key_rsb = f"{S3_PQ_PREFIX_RSB}/{date_yyyymmdd}/{time_hhmm}.parquet"
            
            s3_client.put_object(
                Bucket=BUCKET_NAME,
                Key=s3_key_rsb,
                Body=buffer_rsb.getvalue(),
                ContentType='application/octet-stream'
            )
            print(f"ğŸ”” ìƒê¶Œ ì¹´í…Œê³ ë¦¬ë³„ ë°ì´í„°ë¥¼ ì €ì¥ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤. : s3://{BUCKET_NAME}/{s3_key_rsb}")
            saved_parquet_paths['rsb_parquet_path'] = f"s3://{BUCKET_NAME}/{s3_key_rsb}"
        else:
            print("ğŸš¨ ì²˜ë¦¬í•  ìƒê¶Œ RSB ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

        processed_history_s3_key = f"{S3_PROCESSED_HISTORY_PREFIX}/commercial.json"
        try:
            upload_processed_history_to_s3(
                s3_client=s3_client,
                bucket_name=BUCKET_NAME,
                s3_key=processed_history_s3_key,
                processed_history_data= data_dict['processed_observed_at_dict']
            )
        except Exception as e:
            print(f"âŒ ìµœì¢… ì²˜ë¦¬ ì´ë ¥ ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise

        return {
            'processed_observed_at_dict' :  data_dict['processed_observed_at_dict'], 
            's3_parquet_paths': saved_parquet_paths
        }


    @task(task_id="load_to_redshift") 
    def load_to_redshift(saved):
        """
        S3ì—ì„œ Parquet íŒŒì¼ì„ Redshift í…Œì´ë¸”ë¡œ COPY INTO ëª…ë ¹ì„ ì‚¬ìš©í•˜ì—¬ ë¡œë“œí•©ë‹ˆë‹¤.
        """
        s3_parquet_paths = saved['s3_parquet_paths']
        commercial_parquet_path = s3_parquet_paths.get('commercial_parquet_path')
        rsb_parquet_path = s3_parquet_paths.get('rsb_parquet_path')

        if not commercial_parquet_path and not rsb_parquet_path:
            print("ğŸš¨ Redshiftë¡œ ë¡œë“œí•  Parquet íŒŒì¼ ê²½ë¡œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

            # Airflow Connectionì—ì„œ Redshift ì—°ê²° ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        conn_obj = None
        conn_obj = BaseHook.get_connection("redshift_conn_id")

        conn = None
        try:
            conn = psycopg2.connect(
                dbname=conn_obj.schema,
                user=conn_obj.login,
                password=conn_obj.password,
                host=conn_obj.host,
                port=int(conn_obj.port) 
            )
            conn.autocommit = True 
            log.info("Redshiftì— ì„±ê³µì ìœ¼ë¡œ ì—°ê²°ë˜ì—ˆìŠµë‹ˆë‹¤.")

            with conn.cursor() as cur:
                if commercial_parquet_path:
                    commercial_table_name = "source.source_commercial" 
                    print(f"ğŸ”„ Redshift í…Œì´ë¸” '{commercial_table_name}'ì— ë°ì´í„° ë¡œë“œ ì‹œì‘...")

                    copy_commercial_sql = f"""
                    COPY {commercial_table_name} (
                        source_id,
                        area_code,
                        area_name,
                        congestion_level,
                        total_payment_count,
                        payment_amount_min,
                        payment_amount_max,
                        male_ratio,
                        female_ratio,
                        age_10s_ratio,
                        age_20s_ratio,
                        age_30s_ratio,
                        age_40s_ratio,
                        age_50s_ratio,
                        age_60s_ratio,
                        individual_consumer_ratio,
                        corporate_consumer_ratio,
                        observed_at,
                        created_at
                    )
                    FROM '{commercial_parquet_path}'
                    IAM_ROLE '{REDSHIFT_IAM_ROLE}'
                    FORMAT AS PARQUET;
                    """
                    cur.execute(copy_commercial_sql)
                    print(f"âœ… ìƒê¶Œ ë°ì´í„°ê°€ Redshift í…Œì´ë¸” '{commercial_table_name}'ì— ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")

                if rsb_parquet_path:
                    rsb_table_name = "source.source_commercial_rsb" 
                    print(f"ğŸ”„ Redshift í…Œì´ë¸” '{rsb_table_name}'ì— ë°ì´í„° ë¡œë“œ ì‹œì‘...")
                    copy_rsb_sql = f"""
                    COPY {rsb_table_name} (
                    source_id,
                    category_large,
                    category_medium,
                    category_congestion_level,
                    category_payment_count,
                    category_payment_min,
                    category_payment_max,
                    merchant_count,
                    merchant_basis_month,
                    observed_at,
                    created_at
                )
                    FROM '{rsb_parquet_path}'
                    IAM_ROLE '{REDSHIFT_IAM_ROLE}'
                    FORMAT AS PARQUET;
                    """
                    cur.execute(copy_rsb_sql)
                    print(f"âœ… ìƒê¶Œ ì¹´í…Œê³ ë¦¬ë³„ ë°ì´í„°ê°€ Redshift í…Œì´ë¸” '{rsb_table_name}'ì— ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")

        except Exception as e:
            print(f"âŒ Redshiftì— ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise
        finally:
            if conn:
                conn.close()
                print("ğŸ—„ï¸ Redshift ì—°ê²°ì´ ë‹«í˜”ìŠµë‹ˆë‹¤.")


    try:
        conn = BaseHook.get_connection("aws_default")
        session = boto3.Session(
            aws_access_key_id=conn.login,
            aws_secret_access_key=conn.password,
            region_name=json.loads(conn.extra)["region_name"]
        )
        s3 = session.client("s3")
        logging.info("S3 í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ.")
    except Exception as e:
        logging.critical(f"S3 í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return
    
    extracted_data = extract_and_transform(s3_client=s3)
    saved = upload_parquet_to_s3(extracted_data, s3)
    load_to_redshift(saved)


commercial_pipeline_dag = commercial_data_pipeline() 
