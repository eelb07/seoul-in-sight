import logging
import hashlib
import json
from io import BytesIO
import decimal
import os
import subprocess

import pendulum
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
import boto3
import botocore.exceptions
import psycopg2

from airflow.decorators import dag, task
from airflow.models import Variable
from airflow.hooks.base import BaseHook

# --- ì„¤ì • ë³€ìˆ˜ ---
BUCKET_NAME = Variable.get("BUCKET_NAME")
S3_PREFIX = Variable.get("S3_PREFIX")
S3_PQ_PREFIX_COMM = Variable.get("S3_PQ_PREFIX_COMM")
S3_PQ_PREFIX_RSB = Variable.get("S3_PQ_PREFIX_RSB")
S3_PROCESSED_HISTORY_PREFIX = Variable.get("S3_PROCESSED_HISTORY_PREFIX")
REDSHIFT_IAM_ROLE = Variable.get("REDSHIFT_IAM_ROLE_ARN")

DBT_PROJECT_DIR = '/opt/airflow/team1_dbt'

log = logging.getLogger(__name__)

# --- í—¬í¼ í•¨ìˆ˜ ---

def generate_source_id(area_code: str, observed_at: str) -> str:
    """
    ê³ ì • ê¸¸ì´ SHA256 í•´ì‹œë¥¼ ì†ŒìŠ¤ IDë¡œ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        area_code (str): ì§€ì—­ ì½”ë“œ.
        observed_at (str): ê´€ì¸¡ ì‹œê°„.

    Returns:
        str: 32ìž SHA256 í•´ì‹œ.
    """
    raw = f"{area_code}_{observed_at}"
    return hashlib.sha256(raw.encode()).hexdigest()[:32] # 32ìžë¦¬ ê³ ì • ê¸¸ì´

def parse_int(val):
    """
    ê°’ì„ ì •ìˆ˜ë¡œ íŒŒì‹±í•˜ê³ , None, ë¹ˆ ë¬¸ìžì—´ ë° float ë³€í™˜ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    íŒŒì‹±í•  ìˆ˜ ì—†ëŠ” ê°’ì€ pd.NAë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    try:
        return int(float(val)) if val not in [None, ""] else pd.NA
    except (ValueError, TypeError):
        return pd.NA

def parse_float(val):
    """
    ê°’ì„ floatìœ¼ë¡œ íŒŒì‹±í•©ë‹ˆë‹¤. íŒŒì‹±í•  ìˆ˜ ì—†ëŠ” ê°’ì€ Noneì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    try:
        return float(val)
    except (ValueError, TypeError):
        return None

def upload_processed_history_to_s3(
    s3_client,
    bucket_name: str,
    s3_key: str,
    processed_history_data: dict # ì—…ë¡œë“œí•  processed_observed_at_dict ë°ì´í„°
):
    """
    ì—…ë°ì´íŠ¸ëœ ì²˜ë¦¬ ì´ë ¥ ë”•ì…”ë„ˆë¦¬ (processed_observed_at_dict)ë¥¼ S3ì— JSON íŒŒì¼ë¡œ ì—…ë¡œë“œí•©ë‹ˆë‹¤.

    Args:
        s3_client: ì´ˆê¸°í™”ëœ boto3 S3 í´ë¼ì´ì–¸íŠ¸ ê°ì²´.
        bucket_name (str): S3 ë²„í‚· ì´ë¦„.
        s3_key (str): ì²˜ë¦¬ ì´ë ¥ íŒŒì¼ì´ ì €ìž¥ë  S3 í‚¤ (ì˜ˆ: "history/processed_observations.json").
        processed_history_data (dict): area_id ë³„ë¡œ ê·¸ë£¹í™”ëœ ì²˜ë¦¬ ì´ë ¥ ë°ì´í„° (processed_observed_at_dict).
    """
    try:
        updated_content_json_string = json.dumps(processed_history_data, indent=4, ensure_ascii=False)

        s3_client.put_object(
            Bucket=bucket_name,
            Key=s3_key,
            Body=updated_content_json_string.encode('utf-8'),
            ContentType='application/json'
        )

        total_records_count = sum(len(observations) for observations in processed_history_data.values())
        log.info(f"âœ… S3ì— {total_records_count}ê°œì˜ ì²˜ë¦¬ ì´ë ¥ì„ ì„±ê³µì ìœ¼ë¡œ ì—…ë°ì´íŠ¸í–ˆìŠµë‹ˆë‹¤: s3://{bucket_name}/{s3_key}")

    except Exception as e:
        log.error(f"âŒ ì²˜ë¦¬ ì´ë ¥ íŒŒì¼ì„ S3ì— ì €ìž¥í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤ (s3://{bucket_name}/{s3_key}): {e}")
        raise

# --- Airflow DAG ì •ì˜ ---

default_args = {
    "owner": "seungalee",
    "email": ["teamfirst.dag.alert@gmail.com"], # ì•Œë¦¼ì„ ë°›ì„ ì´ë©”ì¼ ì£¼ì†Œ ëª©ë¡ ( í–¥í›„ ì ìš© ê°€ëŠ¥. ë¡œì»¬ì—ì„  ì•ˆë¨)
    "email_on_failure": True,
}

@dag(
    dag_id="dag_commercial_v1.0.0",
    schedule="*/5 * * * *",
    start_date=pendulum.datetime(2025, 7, 2, tz="Asia/Seoul"),
    catchup=False,
    doc_md="""
    # ìƒê¶Œ ë°ì´í„° ETL íŒŒì´í”„ë¼ì¸
    - **ì¶”ì¶œ ë° ë³€í™˜**: S3ì—ì„œ ì›ì‹œ JSON ë°ì´í„°ë¥¼ ì¶”ì¶œ, ë³€í™˜í•˜ê³  ì´ë¯¸ ì²˜ë¦¬ëœ ë ˆì½”ë“œë¥¼ í•„í„°ë§í•©ë‹ˆë‹¤.
    - **Parquet ì—…ë¡œë“œ**: ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ Parquet íŒŒì¼ë¡œ S3ì— ì—…ë¡œë“œí•©ë‹ˆë‹¤.
    - **Redshift ë¡œë“œ**: S3ì˜ Parquet íŒŒì¼ì„ Redshift í…Œì´ë¸”ë¡œ ë¡œë“œí•©ë‹ˆë‹¤.
    - **dbt ëª¨ë¸ ì‹¤í–‰**: ì¦ë¶„ ë¡œë”© ë° ë°ì´í„° ë³€í™˜ì„ ìœ„í•´ dbt ëª¨ë¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
    """,
    tags=["seoul", "commercial", "ETL"],
    default_args=default_args
)
def commercial_data_pipeline():

    @task(task_id="extract_and_transform")
    def extract_and_transform(s3_client):
        """
        S3ì—ì„œ ì›ì‹œ ìƒê¶Œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ê³  ë³€í™˜í•˜ë©°, S3ì˜ ì´ë ¥ íŒŒì¼ì„ ê¸°ë°˜ìœ¼ë¡œ
        ì´ë¯¸ ì²˜ë¦¬ëœ ë ˆì½”ë“œë¥¼ í•„í„°ë§í•©ë‹ˆë‹¤.
        """
        process_start_time = pendulum.datetime(2025, 7, 3, 0, 20, tz="Asia/Seoul")
        start_time_for_files = process_start_time.subtract(minutes=5)
        log.info(f"ðŸ””{start_time_for_files} ~ {process_start_time} ì‚¬ì´ì˜ raw_json ì²˜ë¦¬ë¥¼ ì‹œìž‘í•©ë‹ˆë‹¤.")

        # ì²˜ë¦¬í•´ì•¼ í•  ì „ì²´ íŒŒì¼ ê²½ë¡œ ì •ì˜
        files_to_process = []
        for i in range(5):
            file_time = start_time_for_files.add(minutes=i)
            file_time_HHmm = file_time.strftime("%H%M")

            for area_id in range(82):
                files_to_process.append({
                    'file_time': file_time,
                    'area_id': area_id,
                    'file_name': f"{file_time_HHmm}_{area_id}.json"
                })

        # S3ì—ì„œ ê¸°ì¡´ ì²˜ë¦¬ ì´ë ¥ ë¡œë“œ
        processed_history_s3_key = f"{S3_PROCESSED_HISTORY_PREFIX}/processed_observations.json"
        processed_observed_at_set = set()
        processed_observed_at_dict = {}
        try:
            response = s3_client.get_object(Bucket=BUCKET_NAME, Key=processed_history_s3_key)
            processed_observed_at_dict = json.loads(response["Body"].read())

            for area_id_str, values in processed_observed_at_dict.items():
                area_id_int = int(area_id_str)
                for value in values:
                    observed_at = value['observed_at']
                    processed_observed_at_set.add((area_id_int, observed_at))
            log.info(f"ðŸ”” S3ì—ì„œ {len(processed_observed_at_set)}ê°œì˜ ê¸°ì¡´ ì²˜ë¦¬ ì´ë ¥ì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")

        except botocore.exceptions.ClientError as e:
            if e.response['Error']['Code'] == "NoSuchKey": # ì²˜ë¦¬ ì´ë ¥ íŒŒì¼(json)ì´ ì—†ëŠ” ê²½ìš°, ì¦‰ ì²« ì‹œìž‘.
                log.warning(f"ðŸš¨ {processed_history_s3_key} ê²½ë¡œì— ê¸°ì¡´ ì²˜ë¦¬ ì´ë ¥ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œ ì‹œìž‘í•©ë‹ˆë‹¤.")
            else:
                raise

        def is_processed(area_id: int, observed_at: str) -> bool:
            """í•´ë‹¹ area_idì™€ observed_at ì¡°í•©ì´ ì´ë¯¸ ì²˜ë¦¬ ë˜ì—ˆëŠ”ì§€ ì—¬ë¶€ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
            return (area_id, observed_at) in processed_observed_at_set

        source_commercial_data = []
        source_commercial_rsb_data = []

        for file_info in files_to_process:
            file_time = file_info['file_time']
            area_id = file_info['area_id']
            file_name = file_info['file_name']

            key = f"{S3_PREFIX}/{file_time.strftime('%Y%m%d')}/{file_name}"

            try:
                response = s3_client.get_object(Bucket=BUCKET_NAME, Key=key)
                content = response["Body"].read()
                raw_data = json.loads(content)
                raw_all_commercial_data = raw_data['LIVE_CMRCL_STTS']
                raw_observed_at = raw_all_commercial_data.get("CMRCL_TIME")

                try:
                    observed_at = pendulum.from_format(
                        raw_observed_at, "YYYYMMDD HHmm", tz="Asia/Seoul"
                    ).format("YYYY-MM-DD HH:mm:ss")
                except Exception as e:
                    log.error(f"ðŸš¨ í•´ë‹¹ ì‹œê°({raw_observed_at}) íŒŒì‹±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì˜¤ë¥˜: {e}")
                    continue

                if is_processed(area_id=area_id, observed_at=observed_at):
                    log.info(f"â­ï¸ í•´ë‹¹ ì‹œê°ì˜ ìƒê¶Œ ë°ì´í„°ëŠ” ì´ë¯¸ ì²˜ë¦¬ ë˜ì—ˆìŠµë‹ˆë‹¤. ìŠ¤í‚µí•©ë‹ˆë‹¤. : {file_name} (area_id: {area_id}, observed_at: {observed_at})")
                    continue

                source_id = generate_source_id(raw_data.get("AREA_CD", ""), observed_at)

                source_commercial_data.append({
                    'source_id': source_id,
                    'area_code': str(raw_data.get("AREA_CD", "")),
                    'area_name': str(raw_data.get("AREA_NM", "")),
                    'congestion_level': str(raw_all_commercial_data.get("AREA_CMRCL_LVL", "")),                         # ìž¥ì†Œ ì‹¤ì‹œê°„ ìƒê¶Œ í˜„í™©
                    'total_payment_count': parse_int(raw_all_commercial_data.get("AREA_SH_PAYMENT_CNT")),               # ìž¥ì†Œ ì‹¤ì‹œê°„ ì‹ í•œì¹´ë“œ ê²°ì œ ê±´ìˆ˜
                    'payment_amount_min': parse_int(raw_all_commercial_data.get("AREA_SH_PAYMENT_AMT_MIN")),            # ìž¥ì†Œ ì‹¤ì‹œê°„ ì‹ í•œì¹´ë“œ ê²°ì œ ìµœì†Œê°’
                    'payment_amount_max': parse_int(raw_all_commercial_data.get("AREA_SH_PAYMENT_AMT_MAX")),            # ìž¥ì†Œ ì‹¤ì‹œê°„ ì‹ í•œì¹´ë“œ ê²°ì œ ìµœëŒ€ê°’
                    'male_ratio': parse_float(raw_all_commercial_data.get("CMRCL_MALE_RATE")),                          # ë‚¨ì„± ì†Œë¹„ ë¹„ìœ¨
                    'female_ratio': parse_float(raw_all_commercial_data.get("CMRCL_FEMALE_RATE")),                      # ì—¬ì„± ì†Œë¹„ ë¹„ìœ¨
                    'age_10s_ratio': parse_float(raw_all_commercial_data.get("CMRCL_10_RATE")),                         # 10ëŒ€ ì†Œë¹„ ë¹„ìœ¨
                    'age_20s_ratio': parse_float(raw_all_commercial_data.get("CMRCL_20_RATE")),                         # 20ëŒ€ ì†Œë¹„ ë¹„ìœ¨
                    'age_30s_ratio': parse_float(raw_all_commercial_data.get("CMRCL_30_RATE")),                         # 30ëŒ€ ì†Œë¹„ ë¹„ìœ¨
                    'age_40s_ratio': parse_float(raw_all_commercial_data.get("CMRCL_40_RATE")),                         # 40ëŒ€ ì†Œë¹„ ë¹„ìœ¨
                    'age_50s_ratio': parse_float(raw_all_commercial_data.get("CMRCL_50_RATE")),                         # 50ëŒ€ ì†Œë¹„ ë¹„ìœ¨
                    'age_60s_ratio': parse_float(raw_all_commercial_data.get("CMRCL_60_RATE")),                         # 60ëŒ€ ì†Œë¹„ ë¹„ìœ¨
                    'individual_consumer_ratio': parse_float(raw_all_commercial_data.get("CMRCL_PERSONAL_RATE")),       # ê°œì¸ ì†Œë¹„ ë¹„ìœ¨
                    'corporate_consumer_ratio': parse_float(raw_all_commercial_data.get("CMRCL_CORPORATION_RATE")),     # ë²•ì¸ ì†Œë¹„ ë¹„ìœ¨
                    'observed_at': observed_at,                                                                         # ì‹¤ì‹œê°„ ìƒê¶Œ ì—…ë°ì´íŠ¸ ì‹œê°„
                    'created_at': pendulum.now("Asia/Seoul").to_datetime_string()                                       # ì ìž¬ ì‹œê°„
                })

                for value in raw_all_commercial_data.get("CMRCL_RSB", []): # ì—¬ëŸ¬ ì¹´í…Œê³ ë¦¬ì˜ ìƒê¶Œ ë°ì´í„°ê°€ ì¡´ìž¬
                    source_commercial_rsb_data.append({
                        'source_id': source_id,
                        'category_large': str(value.get("RSB_LRG_CTGR", "")),                                           # ì—…ì¢… ëŒ€ë¶„ë¥˜
                        'category_medium': str(value.get("RSB_MID_CTGR", "")),                                          # ì—…ì¢… ì¤‘ë¶„ë¥˜
                        'category_congestion_level': str(value.get("RSB_PAYMENT_LVL", "")),                             # ì—…ì¢… ì‹¤ì‹œê°„ ìƒê¶Œ í˜„í™©
                        'category_payment_count': parse_int(value.get("RSB_SH_PAYMENT_CNT")),                           # ì—…ì¢… ì‹¤ì‹œê°„ ì‹ í•œì¹´ë“œ ê²°ì œ ê±´ìˆ˜
                        'category_payment_min': parse_int(value.get("RSB_SH_PAYMENT_AMT_MIN")),                         # ì—…ì¢… ì‹¤ì‹œê°„ ì‹ í•œì¹´ë“œ ê²°ì œ ê¸ˆì•¡ ìµœì†Œê°’
                        'category_payment_max': parse_int(value.get("RSB_SH_PAYMENT_AMT_MAX")),                         # ì—…ì¢… ì‹¤ì‹œê°„ ì‹ í•œì¹´ë“œ ê²°ì œ ê¸ˆì•¡ ìµœëŒ€ê°’
                        'merchant_count': parse_int(value.get("RSB_MCT_CNT")),                                          # ì—…ì¢… ê°€ë§¹ì  ìˆ˜
                        'merchant_basis_month': str(value.get("RSB_MCT_TIME", "")),                                     # ì—…ì¢… ê°€ë§¹ì  ìˆ˜ ì—…ë°ì´íŠ¸ ì›”
                        'observed_at': observed_at,                                                                     # ì‹¤ì‹œê°„ ìƒê¶Œ í˜„í™© ì—…ë°ì´íŠ¸ ì‹œê°„
                        'created_at': pendulum.now("Asia/Seoul").to_datetime_string()                                   # ì ìž¬ì‹œê°„
                    })
                log.info(f"ðŸ”” {file_name}ì˜ ì „ì²˜ë¦¬ë¥¼ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.")

                # ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ëœ ë°ì´í„°ëŠ” ì´ë ¥ setì— ì¶”ê°€
                processed_observed_at_set.add((area_id, observed_at))
                # S3ì— ì €ìž¥í•  ì´ë ¥ ë”•ì…”ë„ˆë¦¬ì—ë„ ì¶”ê°€
                if str(area_id) not in processed_observed_at_dict:
                    processed_observed_at_dict[str(area_id)] = []
                processed_observed_at_dict[str(area_id)].append({
                    'observed_at': observed_at,
                    'processed_at': pendulum.now("Asia/Seoul").to_datetime_string()
                })

            except botocore.exceptions.ClientError as e:
                if e.response['Error']['Code'] == "NoSuchKey":
                    log.info(f"ðŸš¨ í•´ë‹¹ ì‹œê°({key})ì˜ íŒŒì¼ì´ ì¡´ìž¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
                    continue
                else:
                    raise

        return {
            'source_commercial_data': source_commercial_data,
            'source_commercial_rsb_data': source_commercial_rsb_data,
            'processed_observed_at_dict': processed_observed_at_dict
        }

    @task(task_id="upload_parquet")
    def upload_parquet_to_s3(data_dict: dict, s3_client):
        """
        ì²˜ë¦¬ëœ ìƒê¶Œ ë°ì´í„°ì™€ RSB ë°ì´í„°ë¥¼ Parquet íŒŒì¼ë¡œ S3ì— ì—…ë¡œë“œí•©ë‹ˆë‹¤.
        """
        commercial_data = data_dict['source_commercial_data']
        commercial_rsb_data = data_dict['source_commercial_rsb_data']

        current_process_time = pendulum.now("Asia/Seoul")
        date_yyyymmdd = current_process_time.strftime('%Y%m%d')
        time_hhmm = current_process_time.strftime('%H%M')

        saved_parquet_paths = {
            'commercial_parquet_path': None,
            'rsb_parquet_path': None
        }

        if commercial_data:
            # ìƒê¶Œ ë°ì´í„°ìš© PyArrow Table ìƒì„±ì„ ìœ„í•œ ì»¬ëŸ¼ë³„ ë¦¬ìŠ¤íŠ¸ (dict of lists)ë¡œ ë³€í™˜
            columns_data = {
                'source_id': [], 'area_code': [], 'area_name': [], 'congestion_level': [],
                'total_payment_count': [], 'payment_amount_min': [], 'payment_amount_max': [],
                'male_ratio': [], 'female_ratio': [], 'age_10s_ratio': [], 'age_20s_ratio': [],
                'age_30s_ratio': [], 'age_40s_ratio': [], 'age_50s_ratio': [], 'age_60s_ratio': [],
                'individual_consumer_ratio': [], 'corporate_consumer_ratio': [],
                'observed_at': [], 'created_at': []
            }

            # ë°ì´í„° íƒ€ìž… ë§¤í•‘ ë° ì²˜ë¦¬
            for row in commercial_data:
                columns_data['source_id'].append(str(row.get("source_id", "")))
                columns_data['area_code'].append(str(row.get("area_code", "")))
                columns_data['area_name'].append(str(row.get("area_name", "")))
                columns_data['congestion_level'].append(str(row.get("congestion_level", "")))

                columns_data['total_payment_count'].append(row.get("total_payment_count"))
                columns_data['payment_amount_min'].append(row.get("payment_amount_min"))
                columns_data['payment_amount_max'].append(row.get("payment_amount_max"))

                for col in [
                    "male_ratio", "female_ratio", "age_10s_ratio", "age_20s_ratio",
                    "age_30s_ratio", "age_40s_ratio", "age_50s_ratio", "age_60s_ratio",
                    "individual_consumer_ratio", "corporate_consumer_ratio"
                ]:
                    val = row.get(col)
                    columns_data[col].append(
                        decimal.Decimal(str(round(float(val), 1))) if not pd.isna(val) and val is not None else None
                    )

                try:
                    columns_data['observed_at'].append(pd.Timestamp(row.get("observed_at")))
                except (ValueError, TypeError):
                    columns_data['observed_at'].append(None)

                try:
                    columns_data['created_at'].append(pd.Timestamp(row.get("created_at")))
                except (ValueError, TypeError):
                    columns_data['created_at'].append(None)

            # PyArrow Schema ì •ì˜
            schema = pa.schema([
                pa.field('source_id', pa.string()),
                pa.field('area_code', pa.string()),
                pa.field('area_name', pa.string()),
                pa.field('congestion_level', pa.string()),
                pa.field('total_payment_count', pa.int32()),
                pa.field('payment_amount_min', pa.int32()),
                pa.field('payment_amount_max', pa.int32()),
                pa.field('male_ratio', pa.decimal128(5, 2)),
                pa.field('female_ratio', pa.decimal128(5, 2)),
                pa.field('age_10s_ratio', pa.decimal128(5, 2)),
                pa.field('age_20s_ratio', pa.decimal128(5, 2)),
                pa.field('age_30s_ratio', pa.decimal128(5, 2)),
                pa.field('age_40s_ratio', pa.decimal128(5, 2)),
                pa.field('age_50s_ratio', pa.decimal128(5, 2)),
                pa.field('age_60s_ratio', pa.decimal128(5, 2)),
                pa.field('individual_consumer_ratio', pa.decimal128(5, 2)),
                pa.field('corporate_consumer_ratio', pa.decimal128(5, 2)),
                pa.field('observed_at', pa.timestamp('s')),
                pa.field('created_at', pa.timestamp('s'))
            ])

            # PyArrow Table ìƒì„± (from_pydict ì‚¬ìš©)
            table = pa.Table.from_pydict(columns_data, schema=schema)

            # Parquet ì €ìž¥
            buffer_commercial = BytesIO()
            pq.write_table(table, buffer_commercial, compression='snappy')

            # S3 ì—…ë¡œë“œ
            s3_key_commercial = f"{S3_PQ_PREFIX_COMM}/{date_yyyymmdd}/{time_hhmm}.parquet"

            s3_client.put_object(
                Bucket=BUCKET_NAME,
                Key=s3_key_commercial,
                Body=buffer_commercial.getvalue(),
                ContentType='application/octet-stream'
            )
            print(f"ðŸ”” ìƒê¶Œ ë°ì´í„°ë¥¼ ì €ìž¥ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤. : s3://{BUCKET_NAME}/{s3_key_commercial}")
            saved_parquet_paths['commercial_parquet_path'] = f"s3://{BUCKET_NAME}/{s3_key_commercial}"

        else:
            print("ðŸš¨ ì²˜ë¦¬í•  ìƒê¶Œ ì¼ë°˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")


        if commercial_rsb_data:
            columns_data_rsb = {
                'source_id': [], 'category_large': [], 'category_medium': [],
                'category_congestion_level': [], 'category_payment_count': [],
                'category_payment_min': [], 'category_payment_max': [],
                'merchant_count': [], 'merchant_basis_month': [],
                'observed_at': [], 'created_at': []
            }

            for row in commercial_rsb_data:
                columns_data_rsb['source_id'].append(str(row.get('source_id', '')))
                columns_data_rsb['category_large'].append(str(row.get('category_large', '')))
                columns_data_rsb['category_medium'].append(str(row.get('category_medium', '')))
                columns_data_rsb['category_congestion_level'].append(str(row.get('category_congestion_level', '')))

                columns_data_rsb['category_payment_count'].append(row.get('category_payment_count'))
                columns_data_rsb['category_payment_min'].append(row.get('category_payment_min'))
                columns_data_rsb['category_payment_max'].append(row.get('category_payment_max'))
                columns_data_rsb['merchant_count'].append(row.get('merchant_count'))
                columns_data_rsb['merchant_basis_month'].append(row.get('merchant_basis_month'))

                try:
                    columns_data_rsb['observed_at'].append(pd.Timestamp(row.get('observed_at')))
                except (ValueError, TypeError):
                    columns_data_rsb['observed_at'].append(None)

                try:
                    columns_data_rsb['created_at'].append(pd.Timestamp(row.get('created_at')))
                except (ValueError, TypeError):
                    columns_data_rsb['created_at'].append(None)

            # PyArrow Schema ì •ì˜
            schema_rsb = pa.schema([
                pa.field('source_id', pa.string()),
                pa.field('category_large', pa.string()),
                pa.field('category_medium', pa.string()),
                pa.field('category_congestion_level', pa.string()),
                pa.field('category_payment_count', pa.int32()),
                pa.field('category_payment_min', pa.int32()),
                pa.field('category_payment_max', pa.int32()),
                pa.field('merchant_count', pa.int32()),
                pa.field('merchant_basis_month', pa.string()),
                pa.field('observed_at', pa.timestamp('s')),
                pa.field('created_at', pa.timestamp('s'))
            ])

            # PyArrow Table ìƒì„±
            table_rsb = pa.Table.from_pydict(columns_data_rsb, schema=schema_rsb)

            # Parquet íŒŒì¼ë¡œ ì €ìž¥
            buffer_rsb = BytesIO()
            pq.write_table(table_rsb, buffer_rsb, compression='snappy')

            # S3ì— ì—…ë¡œë“œ
            s3_key_rsb = f"{S3_PQ_PREFIX_RSB}/{date_yyyymmdd}/{time_hhmm}.parquet"

            s3_client.put_object(
                Bucket=BUCKET_NAME,
                Key=s3_key_rsb,
                Body=buffer_rsb.getvalue(),
                ContentType='application/octet-stream'
            )
            print(f"ðŸ”” ìƒê¶Œ ì¹´í…Œê³ ë¦¬ë³„ ë°ì´í„°ë¥¼ ì €ìž¥ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤. : s3://{BUCKET_NAME}/{s3_key_rsb}")
            saved_parquet_paths['rsb_parquet_path'] = f"s3://{BUCKET_NAME}/{s3_key_rsb}"
        else:
            print("ðŸš¨ ì²˜ë¦¬í•  ìƒê¶Œ RSB ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

        processed_history_s3_key = f"{S3_PROCESSED_HISTORY_PREFIX}/commercial.json"
        try:
            upload_processed_history_to_s3(
                s3_client=s3_client,
                bucket_name=BUCKET_NAME,
                s3_key=processed_history_s3_key,
                processed_history_data=data_dict['processed_observed_at_dict']
            )
        except Exception as e:
            print(f"âŒ ìµœì¢… ì²˜ë¦¬ ì´ë ¥ ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise

        return {
            'processed_observed_at_dict': data_dict['processed_observed_at_dict'],
            's3_parquet_paths': saved_parquet_paths
        }


    @task(task_id="load_to_redshift")
    def load_to_redshift(saved):
        """
        S3ì—ì„œ Parquet íŒŒì¼ì„ Redshift í…Œì´ë¸”ë¡œ COPY INTO ëª…ë ¹ì„ ì‚¬ìš©í•˜ì—¬ ë¡œë“œí•©ë‹ˆë‹¤.
        """
        s3_parquet_paths = saved['s3_parquet_paths']
        commercial_parquet_path = s3_parquet_paths.get('commercial_parquet_path')
        rsb_parquet_path = s3_parquet_paths.get('rsb_parquet_path')

        if not commercial_parquet_path and not rsb_parquet_path:
            print("ðŸš¨ Redshiftë¡œ ë¡œë“œí•  Parquet íŒŒì¼ ê²½ë¡œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        conn = None
        try:
            # Airflow Connectionì—ì„œ Redshift ì—°ê²° ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            conn_obj = BaseHook.get_connection("redshift_conn_id")
            conn = psycopg2.connect(
                dbname=conn_obj.schema,
                user=conn_obj.login,
                password=conn_obj.password,
                host=conn_obj.host,
                port=int(conn_obj.port)
            )
            conn.autocommit = True
            log.info("Redshiftì— ì„±ê³µì ìœ¼ë¡œ ì—°ê²°ë˜ì—ˆìŠµë‹ˆë‹¤.")

            with conn.cursor() as cur:
                if commercial_parquet_path:
                    commercial_table_name = "source.source_commercial"
                    print(f"ðŸ”„ Redshift í…Œì´ë¸” '{commercial_table_name}'ì— ë°ì´í„° ë¡œë“œ ì‹œìž‘...")

                    copy_commercial_sql = f"""
                    COPY {commercial_table_name} (
                        source_id, area_code, area_name, congestion_level,
                        total_payment_count, payment_amount_min, payment_amount_max,
                        male_ratio, female_ratio, age_10s_ratio, age_20s_ratio,
                        age_30s_ratio, age_40s_ratio, age_50s_ratio, age_60s_ratio,
                        individual_consumer_ratio, corporate_consumer_ratio,
                        observed_at, created_at
                    )
                    FROM '{commercial_parquet_path}'
                    IAM_ROLE '{REDSHIFT_IAM_ROLE}'
                    FORMAT AS PARQUET;
                    """
                    cur.execute(copy_commercial_sql)
                    print(f"âœ… ìƒê¶Œ ë°ì´í„°ê°€ Redshift í…Œì´ë¸” '{commercial_table_name}'ì— ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")

                if rsb_parquet_path:
                    rsb_table_name = "source.source_commercial_rsb"
                    print(f"ðŸ”„ Redshift í…Œì´ë¸” '{rsb_table_name}'ì— ë°ì´í„° ë¡œë“œ ì‹œìž‘...")
                    copy_rsb_sql = f"""
                    COPY {rsb_table_name} (
                        source_id, category_large, category_medium, category_congestion_level,
                        category_payment_count, category_payment_min, category_payment_max,
                        merchant_count, merchant_basis_month, observed_at, created_at
                    )
                    FROM '{rsb_parquet_path}'
                    IAM_ROLE '{REDSHIFT_IAM_ROLE}'
                    FORMAT AS PARQUET;
                    """
                    cur.execute(copy_rsb_sql)
                    print(f"âœ… ìƒê¶Œ ì¹´í…Œê³ ë¦¬ë³„ ë°ì´í„°ê°€ Redshift í…Œì´ë¸” '{rsb_table_name}'ì— ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")

        except Exception as e:
            print(f"âŒ Redshiftì— ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise
        finally:
            if conn:
                conn.close()
                print("ðŸ—„ï¸ Redshift ì—°ê²°ì´ ë‹«í˜”ìŠµë‹ˆë‹¤.")

    @task(task_id="run_dbt_models")
    def run_dbt_command(command_args: str):
        """
        dbt ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ëŠ” Airflow íƒœìŠ¤í¬.
        ì§€ì •ëœ dbt í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ë¡œ ì´ë™í•˜ì—¬ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
        """
        original_cwd = os.getcwd() # í˜„ìž¬ ìž‘ì—… ë””ë ‰í† ë¦¬ ì €ìž¥
        try:
            os.chdir(DBT_PROJECT_DIR) # dbt í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ë¡œ ì´ë™

            dbt_command = ['dbt'] + command_args.split()

            log.info(f"ðŸš€ dbt command ì‹¤í–‰: {' '.join(dbt_command)}")
            process = subprocess.run(dbt_command, capture_output=True, text=True, check=True)

            log.info("dbt stdout:")
            log.info(process.stdout)
            if process.stderr:
                log.warning("dbt stderr:")
                log.warning(process.stderr)

            return process.stdout

        except subprocess.CalledProcessError as e:
            log.error(f"âŒ dbt command ì‹¤íŒ¨: {e}")
            log.error(f"stdout: {e.stdout}")
            log.error(f"stderr: {e.stderr}")
            raise
        finally:
            os.chdir(original_cwd) # ì›ëž˜ ìž‘ì—… ë””ë ‰í† ë¦¬ë¡œ ëŒì•„ì˜´


    # S3 í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    s3 = None
    try:
        conn = BaseHook.get_connection("aws_default")
        session = boto3.Session(
            aws_access_key_id=conn.login,
            aws_secret_access_key=conn.password,
            region_name=json.loads(conn.extra)["region_name"]
        )
        s3 = session.client("s3")
        logging.info("S3 í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ.")
    except Exception as e:
        logging.critical(f"S3 í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return 

    # ë°ì´í„° ì¶”ì¶œ ë° ì „ì²˜ë¦¬
    extracted_data = extract_and_transform(s3_client=s3)

    # Parquetìœ¼ë¡œ S3ì— ì—…ë¡œë“œ
    saved_paths = upload_parquet_to_s3(extracted_data, s3)

    # Redshift Source í…Œì´ë¸”ë¡œ ë¡œë“œ
    redshift_load_status = load_to_redshift(saved_paths) 

    # Redshift ë¡œë“œ ì™„ë£Œ í›„ dbt ëª¨ë¸ ì‹¤í–‰
    dbt_run_status = run_dbt_command(command_args='run')

    # íƒœìŠ¤í¬ ì˜ì¡´ì„± ì„¤ì •
    extracted_data >> saved_paths >> redshift_load_status >> dbt_run_status

commercial_pipeline_dag = commercial_data_pipeline()