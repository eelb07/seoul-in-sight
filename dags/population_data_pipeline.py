import json
import logging
import pandas as pd
import pendulum
import botocore.exceptions
import io
import textwrap

from typing import List
from airflow.decorators import dag, task
from airflow.models import Variable
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from airflow.providers.amazon.aws.hooks.redshift_sql import RedshiftSQLHook
from airflow.operators.bash import BashOperator

logger = logging.getLogger()
S3_BUCKET_NAME = "de6-team1-bucket"
DBT_PROJECT_DIR = Variable.get("DBT_PROJECT_DIR")
DBT_PROFILES_DIR = Variable.get("DBT_PROFILES_DIR")
REDSHIFT_IAM_ROLE = Variable.get("REDSHIFT_IAM_ROLE")


def get_s3_client(conn_id="aws_conn_id"):
    """airflowì— ë“±ë¡ëœ aws_conn_idë¥¼ ì‚¬ìš©í•˜ì—¬ S3 í´ë¼ì´ì–¸íŠ¸ ë°˜í™˜"""
    s3_hook = S3Hook(aws_conn_id=conn_id)
    return s3_hook.get_conn()


def read_from_s3(s3_client, bucket_name: str, key: str):
    """ì§€ì •í•œ S3 ë²„í‚·ê³¼ í‚¤ì— í•´ë‹¹í•˜ëŠ” ê°ì²´ë¥¼ ë¬¸ìžì—´ë¡œ ë°˜í™˜"""
    response = s3_client.get_object(Bucket=bucket_name, Key=key)
    return response["Body"].read().decode("utf-8")


"""ì²˜ë¦¬ëœ observed_at ê°’ì´ ê¸°ë¡ëœ s3_key(population.json)ì— processed_history_data ì—…ë°ì´íŠ¸"""


def upload_processed_history_to_s3(
    s3_client,
    bucket_name: str,
    s3_key: str,
    processed_history_data: dict,  # area_code ë³„ë¡œ ì²˜ë¦¬ëœ observed_at ê°’ ëª©ë¡ì„ ë‹´ì€ ë”•ì…”ë„ˆë¦¬
):
    updated_content_json_string = json.dumps(
        processed_history_data, indent=4, ensure_ascii=False
    )

    s3_client.put_object(
        Bucket=bucket_name,
        Key=s3_key,
        Body=updated_content_json_string.encode("utf-8"),
        ContentType="application/json",
    )

    total_records_count = sum(
        len(observations) for observations in processed_history_data.values()
    )
    logger.info(
        f"âœ… S3ì— {total_records_count}ê°œì˜ ì²˜ë¦¬ ì´ë ¥ì„ ì„±ê³µì ìœ¼ë¡œ ì—…ë°ì´íŠ¸í–ˆìŠµë‹ˆë‹¤: s3://{bucket_name}/{s3_key}"
    )


@dag(
    dag_id="dag_population",
    schedule="*/5 * * * *",
    start_date=pendulum.datetime(2025, 7, 3, 0, 5, tz="Asia/Seoul"),
    doc_md=textwrap.dedent("""
        - **ì¶”ì¶œ ë° ë³€í™˜**: S3ì—ì„œ raw json ë°ì´í„°ë¥¼ ì¶”ì¶œ, ë³€í™˜í•˜ê³  ì¤‘ë³µ ì²˜ë¦¬
        - **Parquet ì—…ë¡œë“œ**: ì²˜ë¦¬ëœ ë°ì´í„°ëŠ” Parquet íŒŒì¼ë¡œ S3ì— ì—…ë¡œë“œ
        - **Redshift ì ìž¬**: S3ì˜ Parquet íŒŒì¼ì„ Redshift í…Œì´ë¸”ì— ì ìž¬
    """),
    catchup=False,
    tags=["population", "silver", "ETL"],
    default_args={"owner": "hyeonuk"},
)
def population_data_pipeline():
    @task
    def extract_and_transform(**context) -> dict:
        """
        S3ì— ìµœê·¼ 5ë¶„ ë™ì•ˆ ìˆ˜ì§‘ëœ raw json ë°ì´í„°ì—ì„œ ì¸êµ¬ ë°ì´í„° ì¶”ì¶œ
        - ex) logical_dateê°€ 20:10ì´ë©´ 20:05~09 ì‚¬ì´ì— ìˆ˜ì§‘ëœ raw json íƒìƒ‰
        """
        logical_date = context["logical_date"]

        process_start_time = logical_date

        s3 = get_s3_client()
        files_to_process = []

        # 5ë¶„ ë‚´ì˜ ìƒì„±ëœ ë°ì´í„° prefix ìƒì„± í›„ ì‹¤ì œ ì¡´ìž¬í•˜ëŠ” ê²ƒë§Œ ê°€ì ¸ì˜´
        for i in range(5):
            file_time = process_start_time.subtract(minutes=(5 - i))

            s3_prefix_date_path = file_time.strftime("%Y%m%d")
            s3_prefix_time_name = file_time.strftime("%H%M")
            full_s3_prefix = f"raw-json/{s3_prefix_date_path}/{s3_prefix_time_name}_"

            response = s3.list_objects_v2(Bucket=S3_BUCKET_NAME, Prefix=full_s3_prefix)

            for obj in response.get("Contents", []):
                file_key = obj["Key"]

                try:
                    s3 = get_s3_client("aws_conn_id")
                    raw_json = read_from_s3(s3, S3_BUCKET_NAME, file_key)
                    raw_data = json.loads(raw_json)

                    citydata = raw_data.get("LIVE_PPLTN_STTS", [])
                    if not citydata:
                        continue

                    observed_at = (
                        citydata[0]
                        .get("PPLTN_TIME", "unknown_time")
                        .replace(":", "")
                        .replace(" ", "")
                    )

                    area_id = int(file_key.split("_")[-1].split(".")[0])

                    files_to_process.append(
                        {
                            "key": file_key,
                            "observed_at": observed_at,
                            "data": citydata,
                            "area_id": area_id,
                            "file_time": file_time,
                            "file_name": f"{s3_prefix_time_name}_{area_id}.json",
                        }
                    )

                    # logger.info(
                    #     f"[DEBUG] Found file: {file_key}, observed_at={observed_at}"
                    # )

                except botocore.exceptions.ClientError as e:
                    if e.response["Error"]["Code"] == "NoSuchKey":
                        logger.warning(f"S3 key not found during read: {file_key}")
                        continue
                    else:
                        raise

        processed_history_s3_key = (
            "processed_history/population.json"  # ì²˜ë¦¬ ë‚´ì—­ì´ ì €ìž¥ë˜ëŠ” í‚¤
        )
        processed_observed_at_set = set()
        processed_observed_at_dict = {}

        try:
            response = s3.get_object(  # ì²˜ë¦¬ëœ observed_atì¸ì§€ íŒë‹¨í•˜ê¸° ìœ„í•´ population.json ë¡œë“œ
                Bucket=S3_BUCKET_NAME, Key=processed_history_s3_key
            )
            processed_observed_at_dict = json.loads(response["Body"].read())

            for area_id_str, values in processed_observed_at_dict.items():
                area_id_int = int(area_id_str)
                for value in values:
                    observed_at = value["observed_at"]
                    processed_observed_at_set.add((area_id_int, observed_at))
            logger.info(
                f"ðŸ”” S3ì—ì„œ {len(processed_observed_at_set)}ê°œì˜ ê¸°ì¡´ ì²˜ë¦¬ ì´ë ¥ì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤."
            )

        except botocore.exceptions.ClientError as e:
            if e.response["Error"]["Code"] == "NoSuchKey":
                logger.warning(
                    f"ðŸš¨ {processed_history_s3_key} ê²½ë¡œì— ê¸°ì¡´ ì²˜ë¦¬ ì´ë ¥ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œ ì‹œìž‘í•©ë‹ˆë‹¤."
                )
            else:
                raise

        # ì´ì „ì— ì²˜ë¦¬ëœ observed_atì¸ì§€ íŒë‹¨
        def is_processed(area_id: int, observed_at: str) -> bool:
            return (area_id, observed_at) in processed_observed_at_set

        # population ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        source_population_data = []

        for file_info in files_to_process:
            file_time = file_info["file_time"]
            area_id = file_info["area_id"]
            file_name = file_info["file_name"]

            key = f"raw-json/{file_time.strftime('%Y%m%d')}/{file_name}"

            try:
                response = s3.get_object(Bucket=S3_BUCKET_NAME, Key=key)
                content = response["Body"].read()
                raw_data = json.loads(content)
                raw_population_data = raw_data["LIVE_PPLTN_STTS"][
                    0
                ]  # âœ… populationì€ ë°°ì—´ ì²« ë²ˆì§¸ ê°’
                raw_observed_at = raw_population_data.get("PPLTN_TIME")

                try:
                    observed_at = pendulum.from_format(
                        raw_observed_at, "YYYY-MM-DD HH:mm", tz="Asia/Seoul"
                    ).format("YYYY-MM-DD HH:mm:ss")
                except Exception as e:
                    logger.error(
                        f"ðŸš¨ í•´ë‹¹ ì‹œê°({raw_observed_at}) íŒŒì‹±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì˜¤ë¥˜: {e}"
                    )
                    continue

                # ì²˜ë¦¬ëœ ì´ë ¥ì´ ìžˆìœ¼ë©´ ìŠ¤í‚µ ì—†ìœ¼ë©´ ì¶”ê°€
                if is_processed(area_id=area_id, observed_at=observed_at):
                    logger.info(
                        f"â­ï¸ ì´ë¯¸ ì²˜ë¦¬ëœ ë°ì´í„° ìŠ¤í‚µ: {file_name} (area_id: {area_id}, observed_at: {observed_at})"
                    )
                    continue

                source_population_data.append(
                    {
                        "area_code": str(raw_population_data.get("AREA_CD", "")),
                        "area_name": str(raw_population_data.get("AREA_NM", "")),
                        "congestion_label": str(
                            raw_population_data.get("AREA_CONGEST_LVL", "")
                        ),
                        "congestion_message": str(
                            raw_population_data.get("AREA_CONGEST_MSG", "")
                        ),
                        "population_min": int(
                            float(raw_population_data.get("AREA_PPLTN_MIN"))
                        ),
                        "population_max": int(
                            float(raw_population_data.get("AREA_PPLTN_MAX"))
                        ),
                        "male_population_ratio": float(
                            raw_population_data.get("MALE_PPLTN_RATE")
                        ),
                        "female_population_ratio": float(
                            raw_population_data.get("FEMALE_PPLTN_RATE")
                        ),
                        "age_0s_ratio": float(raw_population_data.get("PPLTN_RATE_0")),
                        "age_10s_ratio": float(
                            raw_population_data.get("PPLTN_RATE_10")
                        ),
                        "age_20s_ratio": float(
                            raw_population_data.get("PPLTN_RATE_20")
                        ),
                        "age_30s_ratio": float(
                            raw_population_data.get("PPLTN_RATE_30")
                        ),
                        "age_40s_ratio": float(
                            raw_population_data.get("PPLTN_RATE_40")
                        ),
                        "age_50s_ratio": float(
                            raw_population_data.get("PPLTN_RATE_50")
                        ),
                        "age_60s_ratio": float(
                            raw_population_data.get("PPLTN_RATE_60")
                        ),
                        "age_70s_ratio": float(
                            raw_population_data.get("PPLTN_RATE_70")
                        ),
                        "resident_ratio": float(
                            raw_population_data.get("RESNT_PPLTN_RATE")
                        ),
                        "non_resident_ratio": float(
                            raw_population_data.get("NON_RESNT_PPLTN_RATE")
                        ),
                        "is_replaced": str(raw_population_data.get("REPLACE_YN"))
                        == "Y",
                        "observed_at": observed_at,
                        "created_at": pendulum.now("Asia/Seoul").to_datetime_string(),
                    }
                )

                # ì²˜ë¦¬ ì´ë ¥(oberved_at) ì¶”ê°€
                processed_observed_at_set.add((area_id, observed_at))
                if str(area_id) not in processed_observed_at_dict:
                    processed_observed_at_dict[str(area_id)] = []
                processed_observed_at_dict[str(area_id)].append(
                    {
                        "observed_at": observed_at,
                        "processed_at": pendulum.now("Asia/Seoul").to_datetime_string(),
                    }
                )

                # logger.info(f"âœ… {file_name} ë³€í™˜ ì™„ë£Œ")

            except botocore.exceptions.ClientError as e:
                if e.response["Error"]["Code"] == "NoSuchKey":
                    logger.info(f"ðŸš¨ íŒŒì¼ì´ ì—†ìŒ. ê±´ë„ˆëœ€: {key}")
                    continue
                else:
                    raise

        return {
            "source_population_data": source_population_data,
            "processed_observed_at_dict": processed_observed_at_dict,
        }

    @task
    def load_to_s3(result: dict):
        """
        ì´ì „ taskì—ì„œ ì²˜ë¦¬ëœ ì´ë ¥ì´ ì—†ëŠ” ë°ì´í„°ë“¤ì€ Parquetìœ¼ë¡œ ë³€í™˜
        """
        source_population_data = result.get("source_population_data", [])

        if not source_population_data:
            logger.info("no population data")
            return ""

        df = pd.DataFrame(source_population_data)

        columns_order = [
            "area_name",
            "area_code",
            "congestion_label",
            "congestion_message",
            "population_min",
            "population_max",
            "male_population_ratio",
            "female_population_ratio",
            "age_0s_ratio",
            "age_10s_ratio",
            "age_20s_ratio",
            "age_30s_ratio",
            "age_40s_ratio",
            "age_50s_ratio",
            "age_60s_ratio",
            "age_70s_ratio",
            "resident_ratio",
            "non_resident_ratio",
            "is_replaced",
            "observed_at",
            "created_at",
        ]

        df = df[columns_order]
        df = df.astype(
            {
                "area_code": "string",
                "area_name": "string",
                "congestion_label": "string",
                "congestion_message": "string",
                "population_min": "Int32",
                "population_max": "Int32",
                "male_population_ratio": "float32",
                "female_population_ratio": "float32",
                "age_0s_ratio": "float32",
                "age_10s_ratio": "float32",
                "age_20s_ratio": "float32",
                "age_30s_ratio": "float32",
                "age_40s_ratio": "float32",
                "age_50s_ratio": "float32",
                "age_60s_ratio": "float32",
                "age_70s_ratio": "float32",
                "resident_ratio": "float32",
                "non_resident_ratio": "float32",
                "is_replaced": "bool",
                "observed_at": "datetime64[ns]",
                "created_at": "datetime64[ns]",
            }
        )
        logger.info(f"population ë°ì´í„° ë³€í™˜ ì™„ë£Œ. rows: {len(df)}")

        buffer = io.BytesIO()
        df.to_parquet(buffer, index=False, engine="pyarrow")
        buffer.seek(0)

        # ë³€í™˜ëœ Parquetë“¤ì„ S3ì— ì €ìž¥
        s3 = get_s3_client()
        merged_key = f"processed-data/population/{pendulum.now('Asia/Seoul').format('YYYYMMDD_HHmm')}.parquet"

        s3.put_object(Bucket=S3_BUCKET_NAME, Key=merged_key, Body=buffer.getvalue())

        logger.info(
            f"âœ… population parquet íŒŒì¼ì„ ì €ìž¥í–ˆìŠµë‹ˆë‹¤: s3://{S3_BUCKET_NAME}/{merged_key}"
        )

        # ì²˜ë¦¬ ì–´ë ¥ ì—…ë°ì´íŠ¸ í›„ S3ì— ë®ì–´ì“°ê¸°
        processed_history_s3_key = "processed_history/population.json"
        try:
            upload_processed_history_to_s3(
                s3_client=s3,
                bucket_name=S3_BUCKET_NAME,
                s3_key=processed_history_s3_key,
                processed_history_data=result["processed_observed_at_dict"],
            )
        except Exception as e:
            logger.info(f"âŒ ìµœì¢… ì²˜ë¦¬ ì´ë ¥ ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise

        return merged_key

    @task
    def load_to_redshift(merged_key: List[str], **context):
        if not merged_key:
            """
            5ë¶„ ì‚¬ì´ ìˆ˜ì§‘ëœ ëª¨ë“  jsonì´ ì´ë¯¸ ì²˜ë¦¬ëìœ¼ë©´ parquetìœ¼ë¡œ ë³€í™˜í•  ê²ƒì´ ì—†ìœ¼ë¯€ë¡œ merged_keyëŠ” ë¹ˆ ë¦¬ìŠ¤íŠ¸ìž„
            ê·¸ëŸ¬ë©´ redshift ì ìž¬ ì‹œ copy_sqlì˜ FROM ë¶€ë¶„ì´ 's3://{S3_BUCKET_NAME}'ê°€ ë˜ë¯€ë¡œ ë²„í‚·ì˜ ëª¨ë“  ë°ì´í„°ë¥¼ redshiftì— ì ìž¬í•˜ë ¤ëŠ” ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë¯€ë¡œ ê·¸ ìƒí™©ì— ëŒ€ë¹„í•œ ì²˜ë¦¬
            """
            logger.info(
                "[load_to_redshift] ì²˜ë¦¬í•  parquet íŒŒì¼ì´ ì—†ì–´ ì ìž¬ë¥¼ ìŠ¤í‚µí•©ë‹ˆë‹¤."
            )
            return "SKIPPED"

        hook = RedshiftSQLHook(redshift_conn_id="redshift_dev_db")
        source_table = "source.source_population"

        utc_time = context["logical_date"]
        kst_time = utc_time.in_timezone("Asia/Seoul")
        end_time = kst_time.format("YYYY-MM-DD HH:mm:ss")
        start_time = kst_time.subtract(minutes=5).format("YYYY-MM-DD HH:mm:ss")

        hook.run("BEGIN")
        hook.run(f"""
        DELETE FROM {source_table}
        WHERE observed_at BETWEEN '{start_time}' AND '{end_time}'
        """)

        copy_sql = f"""
            COPY {source_table}
            FROM 's3://{S3_BUCKET_NAME}/{merged_key}'
            IAM_ROLE '{REDSHIFT_IAM_ROLE}'
            FORMAT PARQUET;
        """

        hook.run(copy_sql)
        hook.run("COMMIT")

    run_dbt = BashOperator(
        task_id="run_dbt",
        bash_command=f"cd {DBT_PROJECT_DIR} && dbt deps && dbt run --project-dir {DBT_PROJECT_DIR} --profiles-dir {DBT_PROFILES_DIR} --select fact_population",
    )

    # DAG ìˆœì„œ ëª…ì‹œ
    extract = extract_and_transform()
    merge = load_to_s3(extract)
    load_to_redshift(merge) >> run_dbt


dag_instance = population_data_pipeline()
