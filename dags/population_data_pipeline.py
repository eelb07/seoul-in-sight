import json
import logging
import pandas as pd
import pendulum
import botocore.exceptions
import io
import textwrap

from typing import List
from airflow.decorators import dag, task
from airflow.models import Variable
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from airflow.providers.amazon.aws.hooks.redshift_sql import RedshiftSQLHook
from airflow.operators.bash import BashOperator

logger = logging.getLogger()
S3_BUCKET_NAME = "de6-team1-bucket"
DBT_PROJECT_DIR = Variable.get("DBT_PROJECT_DIR")
DBT_PROFILES_DIR = Variable.get("DBT_PROFILES_DIR")
REDSHIFT_IAM_ROLE = Variable.get("REDSHIFT_IAM_ROLE")


def get_s3_client(conn_id="aws_conn_id"):
    """airflowÏóê Îì±Î°ùÎêú aws_conn_idÎ•º ÏÇ¨Ïö©ÌïòÏó¨ S3 ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ Î∞òÌôò"""
    s3_hook = S3Hook(aws_conn_id=conn_id)
    return s3_hook.get_conn()


def read_from_s3(s3_client, bucket_name: str, key: str):
    """ÏßÄÏ†ïÌïú S3 Î≤ÑÌÇ∑Í≥º ÌÇ§Ïóê Ìï¥ÎãπÌïòÎäî Í∞ùÏ≤¥Î•º Î¨∏ÏûêÏó¥Î°ú Î∞òÌôò"""
    response = s3_client.get_object(Bucket=bucket_name, Key=key)
    return response["Body"].read().decode("utf-8")


"""Ï≤òÎ¶¨Îêú observed_at Í∞íÏù¥ Í∏∞Î°ùÎêú s3_key(population.json)Ïóê processed_history_data ÏóÖÎç∞Ïù¥Ìä∏"""


def upload_processed_history_to_s3(
    s3_client,
    bucket_name: str,
    s3_key: str,
    processed_history_data: dict,  # area_code Î≥ÑÎ°ú Ï≤òÎ¶¨Îêú observed_at Í∞í Î™©Î°ùÏùÑ Îã¥ÏùÄ ÎîïÏÖîÎÑàÎ¶¨
):
    updated_content_json_string = json.dumps(
        processed_history_data, indent=4, ensure_ascii=False
    )

    s3_client.put_object(
        Bucket=bucket_name,
        Key=s3_key,
        Body=updated_content_json_string.encode("utf-8"),
        ContentType="application/json",
    )

    total_records_count = sum(
        len(observations) for observations in processed_history_data.values()
    )
    logger.info(
        f"‚úÖ S3Ïóê {total_records_count}Í∞úÏùò Ï≤òÎ¶¨ Ïù¥Î†•ÏùÑ ÏÑ±Í≥µÏ†ÅÏúºÎ°ú ÏóÖÎç∞Ïù¥Ìä∏ÌñàÏäµÎãàÎã§: s3://{bucket_name}/{s3_key}"
    )


@dag(
    dag_id="dag_population",
    schedule="*/5 * * * *",
    start_date=pendulum.datetime(2025, 7, 3, 0, 5, tz="Asia/Seoul"),
    doc_md=textwrap.dedent("""
        - **Ï∂îÏ∂ú Î∞è Î≥ÄÌôò**: S3ÏóêÏÑú raw json Îç∞Ïù¥ÌÑ∞Î•º Ï∂îÏ∂ú, Î≥ÄÌôòÌïòÍ≥† Ï§ëÎ≥µ Ï≤òÎ¶¨
        - **Parquet ÏóÖÎ°úÎìú**: Ï≤òÎ¶¨Îêú Îç∞Ïù¥ÌÑ∞Îäî Parquet ÌååÏùºÎ°ú S3Ïóê ÏóÖÎ°úÎìú
        - **Redshift Ï†ÅÏû¨**: S3Ïùò Parquet ÌååÏùºÏùÑ Redshift ÌÖåÏù¥Î∏îÏóê Ï†ÅÏû¨
    """),
    catchup=False,
    tags=["population", "silver", "ETL"],
    default_args={"owner": "hyeonuk"},
)
def population_data_pipeline():
    @task
    def extract_and_transform(**context) -> dict:
        """
        S3Ïóê ÏµúÍ∑º 5Î∂Ñ ÎèôÏïà ÏàòÏßëÎêú raw json Îç∞Ïù¥ÌÑ∞ÏóêÏÑú Ïù∏Íµ¨ Îç∞Ïù¥ÌÑ∞ Ï∂îÏ∂ú
        - ex) logical_dateÍ∞Ä 20:10Ïù¥Î©¥ 20:05~09 ÏÇ¨Ïù¥Ïóê ÏàòÏßëÎêú raw json ÌÉêÏÉâ
        """
        logical_date = context["logical_date"]

        process_start_time = logical_date

        s3 = get_s3_client()
        files_to_process = []

        # 5Î∂Ñ ÎÇ¥Ïùò ÏÉùÏÑ±Îêú Îç∞Ïù¥ÌÑ∞ prefix ÏÉùÏÑ± ÌõÑ Ïã§Ï†ú Ï°¥Ïû¨ÌïòÎäî Í≤ÉÎßå Í∞ÄÏ†∏Ïò¥
        for i in range(5):
            file_time = process_start_time.subtract(minutes=(5 - i))

            s3_prefix_date_path = file_time.strftime("%Y%m%d")
            s3_prefix_time_name = file_time.strftime("%H%M")
            full_s3_prefix = f"raw-json/{s3_prefix_date_path}/{s3_prefix_time_name}_"

            response = s3.list_objects_v2(Bucket=S3_BUCKET_NAME, Prefix=full_s3_prefix)

            for obj in response.get("Contents", []):
                file_key = obj["Key"]

                try:
                    s3 = get_s3_client("aws_conn_id")
                    raw_json = read_from_s3(s3, S3_BUCKET_NAME, file_key)
                    raw_data = json.loads(raw_json)

                    citydata = raw_data.get("LIVE_PPLTN_STTS", [])
                    if not citydata:
                        continue

                    observed_at = (
                        citydata[0]
                        .get("PPLTN_TIME", "unknown_time")
                        .replace(":", "")
                        .replace(" ", "")
                    )

                    area_id = int(file_key.split("_")[-1].split(".")[0])

                    files_to_process.append(
                        {
                            "key": file_key,
                            "observed_at": observed_at,
                            "data": citydata,
                            "area_id": area_id,
                            "file_time": file_time,
                            "file_name": f"{s3_prefix_time_name}_{area_id}.json",
                        }
                    )

                    # logger.info(
                    #     f"[DEBUG] Found file: {file_key}, observed_at={observed_at}"
                    # )

                except botocore.exceptions.ClientError as e:
                    if e.response["Error"]["Code"] == "NoSuchKey":
                        logger.warning(f"S3 key not found during read: {file_key}")
                        continue
                    else:
                        raise

        processed_history_s3_key = (
            "processed_history/population.json"  # Ï≤òÎ¶¨ ÎÇ¥Ïó≠Ïù¥ Ï†ÄÏû•ÎêòÎäî ÌÇ§
        )
        processed_observed_at_set = set()
        processed_observed_at_dict = {}

        try:
            response = s3.get_object(  # Ï≤òÎ¶¨Îêú observed_atÏù∏ÏßÄ ÌåêÎã®ÌïòÍ∏∞ ÏúÑÌï¥ population.json Î°úÎìú
                Bucket=S3_BUCKET_NAME, Key=processed_history_s3_key
            )
            processed_observed_at_dict = json.loads(response["Body"].read())

            for area_id_str, values in processed_observed_at_dict.items():
                area_id_int = int(area_id_str)
                for value in values:
                    observed_at = value["observed_at"]
                    processed_observed_at_set.add((area_id_int, observed_at))
            logger.info(
                f"üîî S3ÏóêÏÑú {len(processed_observed_at_set)}Í∞úÏùò Í∏∞Ï°¥ Ï≤òÎ¶¨ Ïù¥Î†•ÏùÑ Î°úÎìúÌñàÏäµÎãàÎã§."
            )

        except botocore.exceptions.ClientError as e:
            if e.response["Error"]["Code"] == "NoSuchKey":
                logger.warning(
                    f"üö® {processed_history_s3_key} Í≤ΩÎ°úÏóê Í∏∞Ï°¥ Ï≤òÎ¶¨ Ïù¥Î†• ÌååÏùºÏù¥ ÏóÜÏäµÎãàÎã§. ÏÉàÎ°ú ÏãúÏûëÌï©ÎãàÎã§."
                )
            else:
                raise

        # Ïù¥Ï†ÑÏóê Ï≤òÎ¶¨Îêú observed_atÏù∏ÏßÄ ÌåêÎã®
        def is_processed(area_id: int, observed_at: str) -> bool:
            return (area_id, observed_at) in processed_observed_at_set

        # population Îç∞Ïù¥ÌÑ∞ Î¶¨Ïä§Ìä∏
        source_population_data = []

        for file_info in files_to_process:
            file_time = file_info["file_time"]
            area_id = file_info["area_id"]
            file_name = file_info["file_name"]

            key = f"raw-json/{file_time.strftime('%Y%m%d')}/{file_name}"

            try:
                response = s3.get_object(Bucket=S3_BUCKET_NAME, Key=key)
                content = response["Body"].read()
                raw_data = json.loads(content)
                raw_population_data = raw_data["LIVE_PPLTN_STTS"][
                    0
                ]  # ‚úÖ populationÏùÄ Î∞∞Ïó¥ Ï≤´ Î≤àÏß∏ Í∞í
                raw_observed_at = raw_population_data.get("PPLTN_TIME")

                try:
                    observed_at = pendulum.from_format(
                        raw_observed_at, "YYYY-MM-DD HH:mm", tz="Asia/Seoul"
                    ).format("YYYY-MM-DD HH:mm:ss")
                except Exception as e:
                    logger.error(
                        f"üö® Ìï¥Îãπ ÏãúÍ∞Å({raw_observed_at}) ÌååÏã±Ïóê Ïã§Ìå®ÌñàÏäµÎãàÎã§. Ïò§Î•ò: {e}"
                    )
                    continue

                # Ï≤òÎ¶¨Îêú Ïù¥Î†•Ïù¥ ÏûàÏúºÎ©¥ Ïä§ÌÇµ ÏóÜÏúºÎ©¥ Ï∂îÍ∞Ä
                if is_processed(area_id=area_id, observed_at=observed_at):
                    logger.info(
                        f"‚è≠Ô∏è Ïù¥ÎØ∏ Ï≤òÎ¶¨Îêú Îç∞Ïù¥ÌÑ∞ Ïä§ÌÇµ: {file_name} (area_id: {area_id}, observed_at: {observed_at})"
                    )
                    continue

                source_population_data.append(
                    {
                        "area_code": str(raw_population_data.get("AREA_CD", "")),
                        "area_name": str(raw_population_data.get("AREA_NM", "")),
                        "congestion_label": str(
                            raw_population_data.get("AREA_CONGEST_LVL", "")
                        ),
                        "congestion_message": str(
                            raw_population_data.get("AREA_CONGEST_MSG", "")
                        ),
                        "population_min": int(
                            float(raw_population_data.get("AREA_PPLTN_MIN"))
                        ),
                        "population_max": int(
                            float(raw_population_data.get("AREA_PPLTN_MAX"))
                        ),
                        "male_population_ratio": float(
                            raw_population_data.get("MALE_PPLTN_RATE")
                        ),
                        "female_population_ratio": float(
                            raw_population_data.get("FEMALE_PPLTN_RATE")
                        ),
                        "age_0s_ratio": float(raw_population_data.get("PPLTN_RATE_0")),
                        "age_10s_ratio": float(
                            raw_population_data.get("PPLTN_RATE_10")
                        ),
                        "age_20s_ratio": float(
                            raw_population_data.get("PPLTN_RATE_20")
                        ),
                        "age_30s_ratio": float(
                            raw_population_data.get("PPLTN_RATE_30")
                        ),
                        "age_40s_ratio": float(
                            raw_population_data.get("PPLTN_RATE_40")
                        ),
                        "age_50s_ratio": float(
                            raw_population_data.get("PPLTN_RATE_50")
                        ),
                        "age_60s_ratio": float(
                            raw_population_data.get("PPLTN_RATE_60")
                        ),
                        "age_70s_ratio": float(
                            raw_population_data.get("PPLTN_RATE_70")
                        ),
                        "resident_ratio": float(
                            raw_population_data.get("RESNT_PPLTN_RATE")
                        ),
                        "non_resident_ratio": float(
                            raw_population_data.get("NON_RESNT_PPLTN_RATE")
                        ),
                        "is_replaced": str(raw_population_data.get("REPLACE_YN"))
                        == "Y",
                        "observed_at": observed_at,
                        "created_at": pendulum.now("Asia/Seoul").to_datetime_string(),
                    }
                )

                # Ï≤òÎ¶¨ Ïù¥Î†•(oberved_at) Ï∂îÍ∞Ä
                processed_observed_at_set.add((area_id, observed_at))
                if str(area_id) not in processed_observed_at_dict:
                    processed_observed_at_dict[str(area_id)] = []
                processed_observed_at_dict[str(area_id)].append(
                    {
                        "observed_at": observed_at,
                        "processed_at": pendulum.now("Asia/Seoul").to_datetime_string(),
                    }
                )

                # logger.info(f"‚úÖ {file_name} Î≥ÄÌôò ÏôÑÎ£å")

            except botocore.exceptions.ClientError as e:
                if e.response["Error"]["Code"] == "NoSuchKey":
                    logger.info(f"üö® ÌååÏùºÏù¥ ÏóÜÏùå. Í±¥ÎÑàÎúÄ: {key}")
                    continue
                else:
                    raise

        return {
            "source_population_data": source_population_data,
            "processed_observed_at_dict": processed_observed_at_dict,
        }

    @task
    def load_to_s3(result: dict):
        """
        Ïù¥Ï†Ñ taskÏóêÏÑú Ï≤òÎ¶¨Îêú Ïù¥Î†•Ïù¥ ÏóÜÎäî Îç∞Ïù¥ÌÑ∞Îì§ÏùÄ ParquetÏúºÎ°ú Î≥ÄÌôò
        """
        source_population_data = result.get("source_population_data", [])

        if not source_population_data:
            logger.info("no population data")
            return ""

        df = pd.DataFrame(source_population_data)

        columns_order = [
            "area_name",
            "area_code",
            "congestion_label",
            "congestion_message",
            "population_min",
            "population_max",
            "male_population_ratio",
            "female_population_ratio",
            "age_0s_ratio",
            "age_10s_ratio",
            "age_20s_ratio",
            "age_30s_ratio",
            "age_40s_ratio",
            "age_50s_ratio",
            "age_60s_ratio",
            "age_70s_ratio",
            "resident_ratio",
            "non_resident_ratio",
            "is_replaced",
            "observed_at",
            "created_at",
        ]

        df = df[columns_order]
        df = df.astype(
            {
                "area_code": "string",
                "area_name": "string",
                "congestion_label": "string",
                "congestion_message": "string",
                "population_min": "Int32",
                "population_max": "Int32",
                "male_population_ratio": "float32",
                "female_population_ratio": "float32",
                "age_0s_ratio": "float32",
                "age_10s_ratio": "float32",
                "age_20s_ratio": "float32",
                "age_30s_ratio": "float32",
                "age_40s_ratio": "float32",
                "age_50s_ratio": "float32",
                "age_60s_ratio": "float32",
                "age_70s_ratio": "float32",
                "resident_ratio": "float32",
                "non_resident_ratio": "float32",
                "is_replaced": "bool",
                "observed_at": "datetime64[ns]",
                "created_at": "datetime64[ns]",
            }
        )
        logger.info(f"population Îç∞Ïù¥ÌÑ∞ Î≥ÄÌôò ÏôÑÎ£å. rows: {len(df)}")

        buffer = io.BytesIO()
        df.to_parquet(buffer, index=False, engine="pyarrow")
        buffer.seek(0)

        # Î≥ÄÌôòÎêú ParquetÎì§ÏùÑ S3Ïóê Ï†ÄÏû•
        s3 = get_s3_client()
        merged_key = f"processed-data/population/{pendulum.now('Asia/Seoul').format('YYYYMMDD_HHmm')}.parquet"

        s3.put_object(Bucket=S3_BUCKET_NAME, Key=merged_key, Body=buffer.getvalue())

        logger.info(
            f"‚úÖ population parquet ÌååÏùºÏùÑ Ï†ÄÏû•ÌñàÏäµÎãàÎã§: s3://{S3_BUCKET_NAME}/{merged_key}"
        )

        # Ï≤òÎ¶¨ Ïñ¥Î†• ÏóÖÎç∞Ïù¥Ìä∏ ÌõÑ S3Ïóê ÎçÆÏñ¥Ïì∞Í∏∞
        processed_history_s3_key = "processed_history/population.json"
        try:
            upload_processed_history_to_s3(
                s3_client=s3,
                bucket_name=S3_BUCKET_NAME,
                s3_key=processed_history_s3_key,
                processed_history_data=result["processed_observed_at_dict"],
            )
        except Exception as e:
            logger.info(f"‚ùå ÏµúÏ¢Ö Ï≤òÎ¶¨ Ïù¥Î†• ÏóÖÎ°úÎìú Ï§ë Ïò§Î•ò Î∞úÏÉù: {e}")
            raise

        return merged_key

    @task
    def load_to_redshift(merged_key: List[str], **context):
        if not merged_key:
            """
            5Î∂Ñ ÏÇ¨Ïù¥ ÏàòÏßëÎêú Î™®Îì† jsonÏù¥ Ïù¥ÎØ∏ Ï≤òÎ¶¨ÎêêÏúºÎ©¥ parquetÏúºÎ°ú Î≥ÄÌôòÌï† Í≤ÉÏù¥ ÏóÜÏúºÎØÄÎ°ú merged_keyÎäî Îπà Î¶¨Ïä§Ìä∏ÏûÑ
            Í∑∏Îü¨Î©¥ redshift Ï†ÅÏû¨ Ïãú copy_sqlÏùò FROM Î∂ÄÎ∂ÑÏù¥ 's3://{S3_BUCKET_NAME}'Í∞Ä ÎêòÎØÄÎ°ú Î≤ÑÌÇ∑Ïùò Î™®Îì† Îç∞Ïù¥ÌÑ∞Î•º redshiftÏóê Ï†ÅÏû¨ÌïòÎ†§Îäî Ïò§Î•òÍ∞Ä Î∞úÏÉùÌïòÎØÄÎ°ú Í∑∏ ÏÉÅÌô©Ïóê ÎåÄÎπÑÌïú Ï≤òÎ¶¨
            """
            logger.info(
                "[load_to_redshift] Ï≤òÎ¶¨Ìï† parquet ÌååÏùºÏù¥ ÏóÜÏñ¥ Ï†ÅÏû¨Î•º Ïä§ÌÇµÌï©ÎãàÎã§."
            )
            return "SKIPPED"

        hook = RedshiftSQLHook(redshift_conn_id="redshift_dev_db")
        source_table = "source.source_population"

        utc_time = context["logical_date"]
        kst_time = utc_time.in_timezone("Asia/Seoul")
        end_time = kst_time.format("YYYY-MM-DD HH:mm:ss")
        start_time = kst_time.subtract(minutes=5).format("YYYY-MM-DD HH:mm:ss")

        hook.run("BEGIN")
        hook.run(f"""
        DELETE FROM {source_table}
        WHERE observed_at BETWEEN '{start_time}' AND '{end_time}'
        """)

        copy_sql = f"""
            COPY {source_table}
            FROM 's3://{S3_BUCKET_NAME}/{merged_key}'
            IAM_ROLE '{REDSHIFT_IAM_ROLE}'
            FORMAT PARQUET;
        """

        hook.run(copy_sql)
        hook.run("COMMIT")

    run_dbt = BashOperator(
        task_id="run_dbt",
        bash_command=f"cd {DBT_PROJECT_DIR} && dbt deps && dbt run --project-dir {DBT_PROJECT_DIR} --profiles-dir {DBT_PROFILES_DIR} --select fact_population",
    )

    # DAG ÏàúÏÑú Î™ÖÏãú
    extract = extract_and_transform()
    merge = load_to_s3(extract)
    load_to_redshift(merge) >> run_dbt


dag_instance = population_data_pipeline()
